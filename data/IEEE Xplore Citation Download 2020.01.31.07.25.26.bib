@INPROCEEDINGS{8663953,
author={G. {Zhao} and Q. {Zhang} and J. {Chu} and Y. {Li} and S. {Liu} and L. {Lin}},
booktitle={2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS)},
title={Augmented Reality Application for Plant Learning},
year={2018},
volume={},
number={},
pages={1108-1111},
abstract={Augmented reality learning resources meet the requirements of contextual and adaptive ubiquitous learning and are gradually favored in the field of education. In our previous work, we proposed a plant knowledge expansion learning system. This system used mobile intelligent terminal to take pictures of plants to get the text, pictures, audio, video and other information related to plants. On the basis of the existing system, this paper further explored the application of augmented reality technology for plant learning, and developed plants augmented reality information display module. This module can be used to scan a specific plant and obtain its 3D model and text information in realtime. Learners can interact with the model by rotating and scaling, which effectively enhances learners' interest in learning.},
keywords={augmented reality;biology computing;botany;computer aided instruction;mobile learning;augmented reality technology;plants augmented reality information display module;augmented reality application;augmented reality learning resources;contextual learning;adaptive ubiquitous learning;plant knowledge expansion learning system;mobile intelligent terminal;contextual learning;Augmented reality;Three-dimensional displays;Solid modeling;Cameras;Education;Learning systems;Feature extraction;plant learning;interactive learning;augmented reality technology (AR)},
doi={10.1109/ICSESS.2018.8663953},
ISSN={2327-0586},
month={Nov},}
@INPROCEEDINGS{7361175,
author={H. {Alhumaidan} and K. P. Y. {Lo} and A. {Selby}},
booktitle={2015 SAI Intelligent Systems Conference (IntelliSys)},
title={Co-design of augmented reality book for collaborative learning experience in primary education},
year={2015},
volume={},
number={},
pages={427-430},
abstract={Through co-design of Augmented Reality (AR) based teaching material, this research aims to enhance collaborative learning experience in primary school education. It will introduce an interactive AR Book based on primary school textbook using tablets as the real time interface. The development of this AR Book employs co-design methods to involve children, teachers, educators and HCI experts from the early stages of the design process. Research insights from the co-design phase will be implemented in the AR Book design. The final outcome of the AR Book will be evaluated in the classroom to explore its effect on the collaborative experience of primary school students. The research aims to answer the question - Can Augmented Books be designed for primary school students in order to support collaboration? This main research question is divided into two sub-questions as follows - How can co-design methods be applied in designing Augmented Book with and for primary school children? And what is the effect of the proposed Augmented Book on primary school students' collaboration? This research will not only present a practical application of co-designing AR Book for and with primary school children, it will also clarify the benefit of AR for education in terms of collaborative experience.},
keywords={augmented reality;computer aided instruction;educational institutions;human computer interaction;notebook computers;teaching;augmented reality book co-design;AR-based teaching material;collaborative learning experience enhancement;primary school education;interactive AR Book;primary school textbook;tablets;real time interface;AR-book design;primary school students;Education;Collaboration;Collaborative work;Human computer interaction;Art;Focusing;Prototypes;Augmented reality;augmented book;co-design;cooperative inquiry;child-computer-interaction;collaborative learning},
doi={10.1109/IntelliSys.2015.7361175},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{7988257,
author={R. {Chang} and Z. {Yu}},
booktitle={2017 International Conference on Applied System Innovation (ICASI)},
title={Application of Augmented Reality technology to promote interactive learning},
year={2017},
volume={},
number={},
pages={1673-1674},
abstract={In recent years, the learning tools based on AR (Augmented Reality) technology have been highly recommended to be applied in educational sites. Teachers display abstract scientific changes in specific images by applying AR technology. By way of applying AR inter-operation to enhance students' interests in learning as well as reduce their cognitive load. This study has applied AR technology to establish a virtual biological laboratory App to be provided for college freshmen to carry out biological experiments as a curriculum preview and experiencing. The content of Virtual biology laboratory App includes units such as virtual microscope, biological anatomy concept, cell division process, and frog's bones. Through the introduction of digital technology into the general biology curriculums are then emerged with AR technology so as to confer how AR technology affects students study effects and biological experimental knowledge recognition. The study has implemented an experiment in object to college freshmen and the experiment results indicate that the integration of AR technology with teaching has made students' attitude towards learning more positive. Through interactive operation and learning, students are better able to master knowledge of fundamental biological experiments. Through the process of the study, the researcher has found the importance that students study scientific knowledge with interactive technology. Consequently, the Institute has designed a virtual biology laboratory App to achieve the benefits of action learning, situational simulation and interactive experiencing.},
keywords={augmented reality;biology computing;computer aided instruction;digital simulation;augmented reality technology;interactive learning;AR technology;educational sites;abstract scientific changes;AR interoperation;student interests;cognitive load reduction;virtual biological laboratory app;college freshmen;biological experiments;curriculum preview;digital technology;general biology curriculums;biological experimental knowledge recognition;student attitude;interactive operation;fundamental biological experiments;interactive technology;action learning;situational simulation;interactive experiencing;Biology;Microscopy;Education;Media;Augmented reality;Tools;Mobile handsets;Augmented Reality;Biology Experimental Curriculum;Interactive learning},
doi={10.1109/ICASI.2017.7988257},
ISSN={null},
month={May},}
@INPROCEEDINGS{8259669,
author={L. {Pombo} and M. M. {Marques}},
booktitle={2017 International Symposium on Computers in Education (SIIE)},
title={Marker-based augmented reality application for mobile learning in an urban park: Steps to make it real under the EduPARK project},
year={2017},
volume={},
number={},
pages={1-5},
abstract={The gap between the use of mobile devices inside and outside school can lead to students' disengagement with learning activities in formal education. To fill this gap, educators can take advantage of mobile devices' dissemination to give students access to educational Augmented reality (AR) systems. However, this type of exploration is relatively new, and researchers are still studying AR's advantages and challenges in education. In that line, the EduPARK project is developing an interactive AR mobile application to support geocaching activities in outdoor environments, thus creating situated learning opportunities. It is to be explored by students and teachers from basic to higher education, but also by the public. The project follows a design-based research methodology, with several cycles of AR application development, user testing and evaluation. This manuscript is a work-in-progress report of the EduPARK project's options regarding the AR content and triggers, and points out some future directions. The EduPARK's option was to use image-based AR, with marker-based tracking, to display mainly botanical content. In a first implementation experience, 74 pupils (aged 9-10 and 13-14) from two schools tested a beta version of the application and AR markers in an urban park. Some technical issues, related to the markers' recognition, were observed and registered by both pupils and monitors, leading to the revision of the markers' purposes, structure, and content. Examples of refined AR markers and content are presented and discussed in this manuscript. Future work will include developing markerless tracking for this application in the selected urban park. Additionally, a proposal for the installation of the refined markers will be presented to the Park's management entity and the fully developed application will be freely offered to the public, promoting the autonomous exploration of this resource. This work is useful for teachers and both educational technology developers and researchers, as an example of how to successfully develop image-based AR for outdoor settings.},
keywords={augmented reality;computer aided instruction;mobile learning;refined AR markers;selected urban park;Park's management entity;autonomous exploration;educational technology developers;augmented reality application;mobile learning;mobile devices;formal education;educational Augmented reality systems;interactive AR mobile application;geocaching activities;higher education;AR application development;user testing;botanical content;EduPARK project;Education;Games;Mobile communication;Mobile handsets;Usability;Augmented reality;Aging;augmented reality;marker-based;mobile learning;science education;outdoor learning environments},
doi={10.1109/SIIE.2017.8259669},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{7732188,
author={N. {Hrishikesh} and J. J. {Nair}},
booktitle={2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
title={Interactive learning system for the hearing impaired and the vocally challenged},
year={2016},
volume={},
number={},
pages={1079-1083},
abstract={In our existing education system, teachers primarily engage students verbally in what we call `chalk and talk' approach. Occasionally, certain learning models are also made use of for the purpose of teaching specific concepts. Smart classroom systems employ PowerPoint presentations, videos and the like. However, lack of sufficient self-interactive models and/or inadequate interaction with them, cause students lose focus. Young children, particularly with disabilities such as those with hearing impairment and vocal dysfunction are prone to it. Our studies showed that students experienced enhanced attentiveness in an environment conducive to self-interactive learning. The word interaction here does not refer to just teacher-student communication rather; it places greater emphasis on interactive self-learning. The student is utmost comfortable when he/she feels to be the center of attention or the teaching is exclusive to him/her. We propose a novel learning system in order to kindle the innate curiosity of students. This article presents an application of the ongoing research on interactive learning. Our system employs both Virtual Reality (VR) and Augmented Reality (AR) to bring about a deeper immersive and effective interactive learning experience to the students. This Interactive VR-AR Learning System (IVRARLS) provides a learning environment with each student being able to independently interact to learn with his or her own virtual learning models in real time. In our scheme, Microsoft Kinect is used for the extraction of interactive gestures of the participant(s). This approach is better suited particularly for the hearing impaired and/or vocally challenged children nevertheless it does not exclusively target them.},
keywords={computer aided instruction;handicapped aids;interactive devices;teaching;virtual reality;interactive learning system;hearing impaired;vocally challenged;education system;chalk and talk approach;specific concept teaching;smart classroom;PowerPoint presentations;videos;vocal dysfunction;enhanced attentiveness;self-interactive learning;teacher-student communication;student innate curiosity;virtual reality;augmented reality;interactive learning experience;interactive VR-AR learning system;IVRARLS;virtual learning models;Microsoft Kinect;interactive gesture extraction;Pipelines;Education;Tracking;Auditory system;Solid modeling;Augmented reality;Augmented reality;virtual reality;hearing impaired;vocally challenged;immersion;interactive learning classrooms;Kinect;Fiducial marker},
doi={10.1109/ICACCI.2016.7732188},
ISSN={null},
month={Sep.},}
@INPROCEEDINGS{7745489,
author={N. {Kommera} and F. {Kaleem} and S. M. S. {Harooni}},
booktitle={2016 IEEE Conference on Intelligence and Security Informatics (ISI)},
title={Smart augmented reality glasses in cybersecurity and forensic education},
year={2016},
volume={},
number={},
pages={279-281},
abstract={Augmented reality is changing the way its users see the world. Smart augmented-reality glasses, with high resolution Optical Head Mounted display, supplements views of the real-world using video, audio, or graphics projected in front of user's eye. The area of Smart Glasses and heads-up display devices is not a new one, however in the last few years, it has seen an extensive growth in various fields including education. Our work takes advantage of a student's ability to adapt to new enabling technologies to investigate improvements teaching techniques in STEM areas and enhance the effectiveness and efficiency in teaching the new course content. In this paper, we propose to focus on the application of Smart Augmented-Reality Glasses in cybersecurity education to attract and retain students in STEM. In addition, creative ways to learn cybersecurity education via Smart Glasses will be explored using a Discovery Learning approach. This mode of delivery will allow students to interact with cybersecurity theories in an innovative, interactive and effective way, enhancing their overall live experience and experimental learning. With the help of collected data and in-depth analysis of existing smart glasses, the ongoing work will lay the groundwork for developing augmented reality applications that will enhance the learning experiences of students. Ultimately, research conducted with the glasses and applications may help to identify the unique skillsets of cybersecurity analysts, learning gaps and learning solutions.},
keywords={augmented reality;computer aided instruction;computer science education;digital forensics;helmet mounted displays;STEM;teaching;smart augmented reality glasses;forensic education;cybersecurity education;STEM;discovery learning;cybersecurity theories;experimental learning;learning experiences;cybersecurity analysts;learning gaps;learning solutions;high resolution optical head mounted display;heads-up display devices;teaching;Glass;Computer security;Forensics;Education;Augmented reality;Mobile handsets;Computers;Smart Augmented-Reality Glasses;cybersecurity education;characteristics of cybersecurity analysts},
doi={10.1109/ISI.2016.7745489},
ISSN={null},
month={Sep.},}
@INPROCEEDINGS{7836534,
author={C. S. C. {Dalim} and T. {Piumsomboon} and A. {Dey} and M. {Billinghurst} and S. {Sunar}},
booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)},
title={TeachAR: An Interactive Augmented Reality Tool for Teaching Basic English to Non-native Children},
year={2016},
volume={},
number={},
pages={344-345},
abstract={TeachAR is an Augmented Reality (AR) tool for teaching English colors, shapes, and spatial relationships to young children aged 4 to 6 years old who are non-native speakers of English. TeachAR utilizes the ARToolkit plugin for the Unity game engine for square marker tracking and game development. The Microsoft Kinect's microphone and speech API is used for isolated word speech recognition, a webcam for image capturing and a desktop monitor for viewing the AR scene. Previous language learning AR applications usually use audio output, however TeachAR uses speech as input for language learning. This paper describes the TeachAR demonstration and user experience with the application.},
keywords={application program interfaces;augmented reality;computer aided instruction;computer games;image capture;linguistics;speech recognition;teaching;interactive augmented reality tool;basic English teaching;nonnative children;TeachAR;AR tool;English colors;English shapes;spatial relationships;young children;nonnative English speakers;ARToolkit plugin;Unity game engine;square marker tracking;game development;Microsoft Kinect microphone;speech API;isolated word speech recognition;webcam;image capturing;desktop monitor;AR scene viewing;language learning;Shape;Speech recognition;Color;Speech;Augmented reality;Education;Games;Augmented Reality;Teaching and Learning;English Language;Children;Non-Native Speakers},
doi={10.1109/ISMAR-Adjunct.2016.0113},
ISSN={null},
month={Sep.},}
@INPROCEEDINGS{8798339,
author={Y. {Chiou} and R. {Barrnaki}},
booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
title={[DC] Learning Tornado Formation via Collaborative Mixed Reality},
year={2019},
volume={},
number={},
pages={1369-1370},
abstract={With the rise of attention to global warming which brings in more extreme weather and climate conditions, the earth science education would be one of the crucial topics for the next generation. Mixed-Reality has been shown to offer more engaging and effective learning solutions on essential science topics, such as math, physics, and chemistry. However, there are few augmented reality and mixed reality applications on earth science subject. Also, collaborative learning has been shown to be beneficial for student learning by aspiring student curiosity, and the ability of cooperation. In this paper, we propose a Mixed Reality Tornado Simulator which offers an earth science education intervention in a collaborative mixed reality setting. Students and their instructor can wear see-through head-mounted displays to cooperate on learning the knowledge of the formation and its damage cause on human-built structures, farming, and vegetation by using our proposed mixed reality application. Also, for evaluating the learning performance in this mixed reality setting, we will study the students cognitive load using standard survey instruments. We will conduct a controlled study with two conditions to compare the proposed intervention in the head-mounted-display setting, versus a desktop setting to test usability and knowledge gain of the students in those settings.},
keywords={augmented reality;cognition;computer aided instruction;geophysics computing;helmet mounted displays;storms;virtual reality;climate conditions;augmented reality;mixed reality application;earth science subject;collaborative learning;student learning;student curiosity;earth science education intervention;collaborative mixed reality setting;learning performance;students cognitive load;head-mounted-display setting;tornado formation;mixed reality tornado simulator;Tornadoes;Virtual reality;Education;Meteorology;Videos;Three-dimensional displays;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Applied computing-Education-Collaborative learning},
doi={10.1109/VR.2019.8798339},
ISSN={2642-5246},
month={March},}
@INPROCEEDINGS{8405457,
author={M. {Ati} and K. {Kabir} and H. {Abdullahi} and M. {Ahmed}},
booktitle={2018 IEEE Symposium on Computer Applications Industrial Electronics (ISCAIE)},
title={Augmented reality enhanced computer aided learning for young children},
year={2018},
volume={},
number={},
pages={129-133},
abstract={Learning to write can be exhausting for young children. In Traditional teaching, children with a different learning abilities are taught with the same rubric. This, in turn, impacts children that need extra attention to catch up with their pairs, which leads children to suffer right from the early learning stages. Traditional teaching methods also are so rigid that makes them unable to automatically identify those children with less abilities and in need of extra work. Hence, with the rapid development of ICT, an innovative learning methods are sought to be important to allow children to be taught with different rubrics. The aim of this research is to improve learning process for pre-school children via introducing Augmented Reality (AR) in the process which, in turn, simplify the learning process as well as identifying children abilities. The research introduces gamification to the process in order to ease the burden on children. Furthermore, we are trying to involve both school as well home to be part of the educational cycle that makes parents to be part of the learning/educational process of their young children. Augmented reality combined with pleasing sound make the learning more interactive and enjoyable. The outcome of this research also helps parents to keep track of their children's learning. The paper also describes the deployment of the application in a local schools as a pilot study so teachers can get feedback on student's learning curve and to fine tune the work further.},
keywords={augmented reality;computer aided instruction;computer games;educational institutions;learning (artificial intelligence);teaching;young children;innovative learning methods;learning process;pre-school children;children abilities;learning abilities;learning stages;teaching methods;augmented reality;computer aided learning;educational process;gamification;educational cycle;students learning curve;Writing;Education;Games;Augmented reality;Mobile applications;Servers;Task analysis;Augmented Reality;Education;Cloud Computing;Mobile Application},
doi={10.1109/ISCAIE.2018.8405457},
ISSN={null},
month={April},}
@INPROCEEDINGS{8843619,
author={I. J. X. {Ang} and K. H. {Lim}},
booktitle={2019 7th International Conference on Smart Computing Communications (ICSCC)},
title={Enhancing STEM Education using Augmented Reality and Machine Learning},
year={2019},
volume={},
number={},
pages={1-5},
abstract={Learning Science, Technology, Engineering and Mathematics (STEM) in the 21st century has been evolved from the conventional textbook to the interactive platform using electronic devices. This paper presents the implementation of a mobile application system, named AUREL (Augmented Reality Learning) in enhancing the learning experience by projecting Augmented Reality (AR) objects onto 2D images. This AR visualization is used to improve the understanding of STEM subjects and increases the enthusiasm of students towards STEM subjects. In this implementation, Google's Cloud Tensor Processing Units (TPUs) are used to train specific datasets alongside Cloud Vision API to detect a wide range of objects. ML Kit for Firebase is used to host the custom TensorFlow Lite models for specific use cases for better accuracy. On the other hand, Google Cloud Platform (GCP) is used to harvest STEM data, manage STEM 3D information and data processing. Subsequently, the processed information will be displayed in AR in the mobile application using ARCore's Sceneform SDK. The application of AUREL could be extended to all science subjects so that students can learn using an interactive platform.},
keywords={application program interfaces;augmented reality;cloud computing;computer aided instruction;computer science education;interactive systems;learning (artificial intelligence);mobile learning;public domain software;STEM;STEM education;Augmented Reality;machine Learning;interactive platform;electronic devices;mobile application system;learning experience;AUREL;STEM 3D information;ARCore's Sceneform SDK;data processing;STEM data;Google Cloud Platform;Cloud Vision API;Google's Cloud Tensor Processing Units;STEM subjects;Cloud computing;Training;Computational modeling;Augmented reality;Google;Mobile applications;Augmented Reality;Machine Learning;STEM Education},
doi={10.1109/ICSCC.2019.8843619},
ISSN={null},
month={June},}
@INPROCEEDINGS{8079771,
author={S. {Sunil} and S. S. K. {Nair}},
booktitle={2017 International Conference on Computer and Applications (ICCA)},
title={An Educational Augmented Reality App To Facilitate Learning Experience},
year={2017},
volume={},
number={},
pages={279-282},
abstract={Augmented Reality is changing education in a dramatic way and it brings a new dimension to teaching and learning practices through amazing visualization of the real world in an interactive environment. The aim of this research is focused at developing a prototype of mobile based Augmented Reality application using Vuforia and Unity which will be helpful and valuable for students in reinforcing their learning experience. Responses from students indicate that this application is very beneficial to improve their learning curiosity and their passion to learn.},
keywords={augmented reality;computer aided instruction;data visualisation;interactive systems;mobile computing;teaching;learning practices;interactive environment;teaching practices;educational augmented reality app;learning experience facilitation;mobile based augmented reality application;real world visualization;Vuforia;Unity;Education;Augmented reality;Three-dimensional displays;Prototypes;Smart phones;Mobile communication;Visualization;Augmented Reality;Unity;Vuforia;Learning;experience},
doi={10.1109/COMAPP.2017.8079771},
ISSN={null},
month={Sep.},}
@INPROCEEDINGS{8590098,
author={P. {Sarkar} and J. S. {Pillai} and A. {Gupta}},
booktitle={2018 IEEE Tenth International Conference on Technology for Education (T4E)},
title={ScholAR: A Collaborative Learning Experience for Rural Schools Using Augmented Reality Application},
year={2018},
volume={},
number={},
pages={8-15},
abstract={Collaborative learning involves working in groups to solve a problem or perform a task. Collaborative learning is often encouraged in rural schools as due to lack of space and infrastructure, students are made to sit on floors and taught together within and between groups. Evidently, in rural schools, technologies have been introduced to support the existing teaching methods. Augmented Reality is one such technology that can provide a collaborative interactive experience. In our study, we provided an AR based application named 'ScholAR' to experimental group of 16 students of 7th grade as at that age they cultivate the ability to reason logically and develop conceptualizing skills. The application involved six tasks on the Mathematics topic of Introduction to 3D Solids targeted to enhance the spatial visualization skills of the students. We did a comparative study with the control group of 16 students who were taught the same topic using physical 3D models and the usual teaching method followed in their school. We report the results of this study, observations and analysis of the use of this AR application in a collaborative environment and the effect of collaboratively using the AR application on the students' performance.},
keywords={augmented reality;computer aided instruction;educational courses;educational institutions;groupware;mathematics computing;teaching;collaborative learning experience;rural schools;collaborative interactive experience;collaborative environment;augmented reality;teaching method;ScholAR application;Mathematics topic;Three-dimensional displays;Education;Shape;Collaborative work;Visualization;Collaboration;Solids;Augmented Reality;Collaborative Learning;Rural Education;ARCore},
doi={10.1109/T4E.2018.00010},
ISSN={null},
month={Dec},}
@INPROCEEDINGS{8569126,
author={W. {Chao} and C. {Yang} and R. {Chang}},
booktitle={2018 1st IEEE International Conference on Knowledge Innovation and Invention (ICKII)},
title={A Study of the Interactive Mathematics Mobile Application Development},
year={2018},
volume={},
number={},
pages={248-249},
abstract={This study has developed an interactive learning App through Augmented Reality (AR) technology. This AR math learning App provides with interactive operations such as combination, stacking, dismantling, multi-angle rotation, and so on. Teachers can apply this App to teach by replacing teaching materials like traditional stack cartons or videos. Students can repeatedly hand-operate and practice through mobile devices to increase the understanding of the concept of space and understanding 3D body. The study has adopted standard way of experiments to implement the learning effects evaluation of 5th grade elementary students and finds there are significant differences in learning outcomes. The study also applies the technology accepting models and semi-structured interviews to understand how teachers and students react after applying the App. The experiments indicate that the AR mathematics learning App developed by the study can make learners easily apply and maintain a positive attitude, which shows it can improve the motivation of learning.},
keywords={augmented reality;computer aided instruction;human factors;mathematics computing;mobile computing;teaching;augmented reality technology;stack cartons;stack videos;AR mathematics learning app;hand-operated devices;grade elementary students;mobile devices;teaching materials;multiangle rotation;interactive mathematics mobile application development;Education;Mathematics;Augmented reality;Learning systems;Technological innovation;Urban areas;Stacking;Interactive learning;Augmented Reality;Mobile App;Mathematics},
doi={10.1109/ICKII.2018.8569126},
ISSN={null},
month={July},}
@INPROCEEDINGS{7359624,
author={J. {Ferguson} and M. {Mentzelopoulos} and A. {Protopsaltis} and D. {Economou}},
booktitle={2015 International Conference on Interactive Mobile Communication Technologies and Learning (IMCL)},
title={Small and flexible web based framework for teaching QR and AR mobile learning application development},
year={2015},
volume={},
number={},
pages={383-385},
abstract={The current paper presents an AR development framework for students to easily make and use small QR linked interactive learning web pages on mobile devices targeted primarily for museums. This framework is extrapolated from an AR UI system using fiducial marker cubes. AR as a platform is just now reaching its full potential. Since smartphones and mobile devices are now at a sufficiently large user base, it is worth looking at the potential for an extremely small form factor delivery system that is flexible, easily modified, and used by educators and students. An easily modifiable AR learning experience will present AR, QR Mobile platform development, interactive museums, and the chosen subject in a new style. This method of flipped learning can be shown to improve not only knowledge of the chosen subject through investigation, but a better understanding of development potentials for the mobile devices now ubiquitous to students.},
keywords={augmented reality;computer aided instruction;computer games;Internet;mobile learning;smart phones;QR code;smart phones;interactive learning;augmented reality mobile learning;QR teaching;flexible Web based framework;small Web based framework;Mobile communication;Augmented reality;Smart phones;Software;Cameras;flipped;mobile learning;Augmented Reality (AR);User Interface (UI);QR},
doi={10.1109/IMCTL.2015.7359624},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8878553,
author={B. {Cahyono} and M. B. {Firdaus} and E. {Budiman} and M. {Wati}},
booktitle={2018 2nd East Indonesia Conference on Computer and Information Technology (EIConCIT)},
title={Augmented Reality Applied to Geometry Education},
year={2018},
volume={},
number={},
pages={299-303},
abstract={Educational media for students to understand 3D geometry is currently conventional, schools and teaching staff find it difficult to get teaching aids as educational media tools to build 3D space. In terms of cognitive aspects, students also find it difficult to understand objects that build 3D space, because without teaching aids they are only able to imagine or imagine themselves building 3D space objects. Augmented reality (AR) is a good medium for deep collaborative simulation. This AR modeling uses marker based where the marker is incorporated in 2D geometry. In this study, we design and build applications of interactive education models on cubes, tubes, conical geometry objects as a means of learning elementary school mathematics. It is expected that this application can be a tool for mathematics teachers in delivering 3D space building material. The results obtained are 3D geometry modeling successfully read the marker and if the 3D AR model is clicked then a formula will appear from each geometry.},
keywords={augmented reality;computer aided instruction;geometry;mathematics computing;teaching;deep collaborative simulation;AR modeling;interactive education models;conical geometry objects;elementary school mathematics;3D space building material;augmented reality;geometry education;teaching staff;teaching aids;educational media tools;cognitive aspects;3D space objects;3D geometry;Geometry;Three-dimensional displays;Media;Education;Information technology;Cameras;augmented reality;geometry;interactive educational model application},
doi={10.1109/EIConCIT.2018.8878553},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8074987,
author={N. S. C. {Babu}},
booktitle={2017 5th National Conference on E-Learning E-Learning Technologies (ELELTECH)},
title={Keynote 1: Internet of Things(IoT) and augmented reality for e-learning},
year={2017},
volume={},
number={},
pages={1-10},
abstract={With the advent of the Internet of Things (IOT) technology, we will be expecting all the smart devices around us to provide various services in real-time. When this IoT technology is combined with Augmented Reality (AR), it will provide end users with real time as well as real world information. One of the prime advantages of AR combining with IoT is the bridging of the gap between the real and digital worlds around us. The ease of downloading features and scope of dynamic usage which AR brings to the smart devices and is expected to bring to the IoT is beyond human imagination! The education sector is expected to gain significantly using these technologies. The goal of technology in education is to make, learning easier, faster and more engaging for the students and to equip the instructor with powerful tools to provide more relevant courseware to their students. With introducing AR in education, we shall be able to provide contextually aware, more relevant, engaging and interactive courseware to the students, which would keep them interested in subject. Also, through IoT, the physical devices can provide context that the AR ecosystem would use for contextually aware rendering of multimedia content. Medical education domain is expected to gain significantly using IOT and AR. It is also observed that the training in the areas of equipment maintenance could gain significantly by applying these technologies judiciously. For eg. The conventional software virtual lab solution does not provide a real feel of performing an experiment. However, using IOT and AR technology, a sensor enabled Hardware based real lab environment can be created for performing the volumetric analysis experiments in chemistry by using WATER instead of Chemicals. C-DAC has developed few AR products in education domain - AR Board, AR Book and AR Game. AR Board is a virtual writing board on which a user can write with a laser pointer. It uses projection based AR Technology by creating a virtual environment for contextual information presentation, and facilitates real-time interaction with the content. AR Book is a standard book that has been instrumented with AR markers printed, which when scanned would show augmented, 3D interactive content, videos, audio and animations contextually, on a printed book, thus adding new dimensionality to it. AR Game is a game based learning mobile application that can be used by students for learning through playing, and for evaluating their knowledge. The technology of these products has been transferred to industry for their further development and proliferation to various educational institutes.},
keywords={augmented reality;biomedical education;computer aided instruction;computer games;Internet of Things;mobile computing;augmented reality;smart devices;IoT technology;education sector;medical education domain;Internet of Things technology;IoT;e-learning;AR technology;AR board;AR book;AR game;game based learning mobile application},
doi={10.1109/ELELTECH.2017.8074987},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{8409241,
author={C. V. {Siang} and M. I. M. {Isham} and F. {Mohamed} and Y. A. {Yusoff} and M. K. {Mokhtar} and B. {Tomi} and A. {Selamat}},
booktitle={2017 IEEE Conference on e-Learning, e-Management and e-Services (IC3e)},
title={Interactive holographic application using augmented reality EduCard and 3D holographic pyramid for interactive and immersive learning},
year={2017},
volume={},
number={},
pages={73-78},
abstract={The advancement of augmented reality (AR) and holographic display technologies have a great potential to support and improve teaching and learning process, because the 3D images give new perspectives to the students to understand certain topic easily and intuitively. In this paper, an Interactive Holographic Display is introduced, which is aimed to ease the teachers to deliver the knowledge to the students as well as to provide self-learning for the students. This proposed solution is implemented using the AR tracking technique and fused with 3D holographic pyramid display. This makes the virtual objects can display in thin air like a real object and makes the holographic effect more realistic and interactive, as the user can interact with the virtual objects using an image target. This paper explains how the system is physically realized in term of hardware configuration and software design.},
keywords={augmented reality;computer aided instruction;holographic displays;interactive devices;object tracking;teaching;three-dimensional displays;Interactive holographic application;augmented reality EduCard;interactive learning;immersive learning;holographic display technologies;teaching;learning process;Interactive Holographic Display;3D holographic pyramid display;virtual objects;holographic effect;image target;3D images;AR tracking technique;hardware configuration;software design;Three-dimensional displays;Cameras;Target tracking;Augmented reality;Software;Smart phones;Glass;Feature-based augmented reality (AR);holographic projection;3D holographic pyramid;human-computer interaction},
doi={10.1109/IC3e.2017.8409241},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8711843,
author={F. {Khalid} and A. I. {Ali} and R. R. {Ali} and M. S. {Bhatti}},
booktitle={2019 International Conference on Engineering and Emerging Technologies (ICEET)},
title={AREd: Anatomy Learning Using Augmented Reality Application},
year={2019},
volume={},
number={},
pages={1-6},
abstract={With the advancements in technology, innovative teaching methodologies are being utilized in the educational environment. Among these, the most promising methodology is Augmented Reality which makes it possible for students to fully understand an abstract concept by representing a visualized and interactive 3D model. This paper gives review of recent studies on AR in different fields of education as well as an overview of some applications that use AR in the field of education to highlight the benefits of utilization of augmented reality. This paper presents an augmented reality application which generates 3D models of anatomy which aims to help students and instructors alike. A survey was conducted with the purpose of exploring acceptance and benefits of this application. The outcomes of survey suggested that students retain what they learned from this application longer compared to traditional teaching methods, students retain concentration from proposed method.},
keywords={augmented reality;biomedical education;computer aided instruction;medical computing;teaching;anatomy learning;augmented reality application;innovative teaching methodologies;educational environment;visualized D model;interactive 3D model;Education;Three-dimensional displays;Augmented reality;Solid modeling;Visualization;Cameras;Chemistry;AR;Augmented Reality;E-learning;Interactive Education;Marker Based Augmented Reality;Innovative teaching methodology;AR Applications;AR education;3D models (three dimensional);4D models (four dimensional)},
doi={10.1109/CEET1.2019.8711843},
ISSN={2409-2983},
month={Feb},}
@INPROCEEDINGS{7096061,
author={G. {Poletti}},
booktitle={2015 IEEE Global Engineering Education Conference (EDUCON)},
title={Work in progress: Mobile technology for teaching in higher education},
year={2015},
volume={},
number={},
pages={789-792},
abstract={This article describes the research and application of new technologies to support classroom lessons. In particular traces the theoretical and applications that are in development and testing in some of the teachings of the degree courses in “Science and Technologies of Communication” and “Science and Technology for Cultural Heritage”. Development of applications, APP and augmented reality, are both a tool for teaching that a development tool that students use to expand their individual skills in the field of multimedia communications, interactive and distributed.},
keywords={augmented reality;computer aided instruction;educational courses;further education;mobile computing;teaching;mobile technology;higher education teaching;augmented reality;classroom lessons;degree courses;Science and Technologies of Communication course;Science and Technology for Cultural Heritage course;Education;Multimedia communication;Mobile handsets;Technological innovation;Media;Streaming media;Augmented reality;augmented reality;ICT;education;interaction},
doi={10.1109/EDUCON.2015.7096061},
ISSN={2165-9559},
month={March},}
@INPROCEEDINGS{8632639,
author={M. K. {Mokhtar} and F. {Mohamed} and M. S. {Sunar} and M. A. M. {Arshad} and M. K. {Mohd Sidik}},
booktitle={2018 IEEE Conference on e-Learning, e-Management and e-Services (IC3e)},
title={Development of Mobile-Based Augmented Reality Colouring for Preschool Learning},
year={2018},
volume={},
number={},
pages={11-16},
abstract={Coloring activity with pens and paper is a natural activity and an important experience for children to practice and express their creative skills. To gain interest and attention of kids making these creative activities are the main challenge faced by teachers and parents. By developing augmented reality coloring to solve this problem require us to design a mobile application with suitable coloring book contents designed suit for kids. This work aims to provide guidelines developing interactive coloring book with augmented reality for kids that integrate with our previous technique. Wonderful Augmented Reality and Arts (wARna) is our previous fast texturing technique as the main core of the framework application that interactively play colored 2D coloring book page by visualizing it in 3D on a user's view of the real world. This work proposed a framework with suitable specification in creation of content so it reusable to create new coloring content that integrated with a mobile application and highlighting issues that need to be solved.},
keywords={augmented reality;computer aided instruction;data visualisation;mobile computing;kids;interactive coloring book;2D coloring book page;coloring content;mobile application;preschool learning;coloring activity;natural activity;creative skills;creative activities;augmented reality coloring;wonderful augmented reality and arts;fast texturing technique;mobile-based augmented reality colouring;coloring book contents;Three-dimensional displays;Solid modeling;Mobile applications;Pipelines;Animals;Rendering (computer graphics);Games;Augmented Reality;Marked-less;Image-based Target;Colouring Activity;Preschool Education},
doi={10.1109/IC3e.2018.8632639},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8783970,
author={C. {Bravo Pillon} and L. {Rocha Machado} and R. P. {da Silva}},
booktitle={2018 XIII Latin American Conference on Learning Technologies (LACLO)},
title={Augmented Reality Application Development About Nutrition for the Elderly},
year={2018},
volume={},
number={},
pages={21-24},
abstract={Each year increases the number of the elderly around the world, and, in this prospect, new challenges come up, both in relation to culture as in health. The misinformation is one of leading causes that contributes for an inappropriate nutrition between the elderly. The educational material plays a large role in raising awareness to adopt a healthy diet. In this way, the aim of this paper is to present the development and application process of an augmented reality application about nutrition for the elderly. The methodology adopted in this research is divided in four stages: research, development, evaluation and results analysis. In this paper, the first two stages of the methodology are emphasized. The results present the development process of a digital educational material aimed at the elderly, which uses the augmented reality technology. Therefore, the application might complement the learning of the elderly about nutrition in a playful, dynamic, and interactive way.},
keywords={augmented reality;computer aided instruction;geriatrics;health care;augmented reality application development;healthy diet;digital educational material;augmented reality technology;elderly people;Senior citizens;Augmented reality;Conferences;Hardware;Resists;Instruments;Assisted living;elderly;nutritional orientation;digital educational material;application;augmented reality},
doi={10.1109/LACLO.2018.00014},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8192219,
author={S. {Touel} and M. {Mekkadem} and M. {Kenoui} and S. {Benbelkacem}},
booktitle={2017 5th International Conference on Electrical Engineering - Boumerdes (ICEE-B)},
title={Collocated learning experience within collaborative augmented environment (anatomy course)},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Nowadays, collaborative systems and applications are widely used to allow multiple users to work together in order to achieve one same goal. In the field of virtual and augmented realities, such collaboration is often sought to effectuate complex activities in some relevant experiences. Users can share virtual objects, but most importantly, they use collaborative 3D interaction, visualize their objects manipulation in real time and communicate with each other to coordinate their actions efficiently. These users will, therefore, be able to work together in a shared virtual environment to carry out different tasks. In this paper, we present our work related to the design and implementation of a collaborative augmented system, our application introduces a multi-users experience in a face-to-face configuration and can be used by the students in anatomy course.},
keywords={augmented reality;computer aided instruction;educational courses;groupware;human factors;augmented realities;virtual objects;collaborative 3D interaction;objects manipulation;shared virtual environment;collaborative augmented system;multiusers experience;anatomy course;collocated learning experience;collaborative augmented environment;collaborative systems;virtual realities;Three-dimensional displays;Augmented reality;Collaboration;Visualization;Protocols;Games;Servers;Augmented reality;Collaborative environment;Learning application;Anatomy course},
doi={10.1109/ICEE-B.2017.8192219},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8474884,
author={G. {Alshi} and M. {Dandiwala} and M. {Cazi} and R. {Pawar}},
booktitle={2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)},
title={Interactive Augmented Reality-Based System for Traditional Educational Media Using Marker-Derived Contextual Overlays},
year={2018},
volume={},
number={},
pages={930-935},
abstract={Augmented reality is making an impact across different sectors of society. Drawing from this fact, the system proposed herein aims to aid educators by making the process of learning interactive, enjoyable and effective. Using this, theoretical contents of books are enhanced using computer vision, 3D models and Human-Computer interaction (HCI). This enhancement is made available to students in the form of a mobile phone application through an AR interface. One can hover over data or information in the book and get the augmented experience in terms of 3D models, audio or videos. Augmented reality is fast becoming an important part of human life with emerging technologies such as tracking techniques, computer vision systems, graphics, mobile computing gaining traction. The proposed approach aims to interesting and motivational compared to old teaching methods. The paper also discusses the educational settings where the suggested model can be used, along with a detailed description of the prototype, its applications, and responses.},
keywords={augmented reality;computer aided instruction;computer graphics;computer vision;human computer interaction;mobile computing;teaching;interactive augmented reality-based system;traditional educational media;marker-derived contextual overlays;mobile phone application;augmented experience;computer vision systems;educational settings;human-computer interaction;book contents;mobile computing;educators;interactive learning;3D models;AR interface;tracking techniques;graphics;teaching method;Image edge detection;Three-dimensional displays;Cameras;Databases;Conferences;Solid modeling;Videos;Augmented Reality;Interactive Learning;Markers;Tradition Teaching Practices;Overlay;3D object},
doi={10.1109/ICECA.2018.8474884},
ISSN={null},
month={March},}
@INPROCEEDINGS{8425430,
author={M. {Abernethy} and O. {Sinnen} and J. {Adams} and G. {De Ruvo} and N. {Giacaman}},
booktitle={2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
title={ParallelAR: An Augmented Reality App and Instructional Approach for Learning Parallel Programming Scheduling Concepts},
year={2018},
volume={},
number={},
pages={324-331},
abstract={Parallel programming has rapidly moved from a special-purpose technique to standard practice. This newfound ubiquity needs to be matched by improved parallel programming education. As parallel programming involves higher level concepts, students tend to struggle with turning the abstract information into concrete mental models. Analogies are known to aid in this knowledge transfer, by providing an existing schema as the basis for the formation of a new schema. Additionally, technology has been proven to increase motivation and engagement in students, which ultimately improves learning. Combining these ideas, this paper presents several contributions that enhance aspects of parallel programming education. These contributions include a set of collaborative learning activities to target fundamental scheduling concepts, a detailed analogy to assist in the understanding of the scheduling concepts, and an augmented reality application to facilitate the collaborative learning activity by bringing the analogy to life.},
keywords={augmented reality;computer aided instruction;computer science education;groupware;parallel programming;instructional approach;learning parallel programming scheduling concepts;concrete mental models;collaborative learning activity;fundamental scheduling concepts;augmented reality application;ParallelAR;parallel programming education;Education;Augmented reality;Collaborative work;Parallel programming;Smart phones;Headphones;Analogies;Augmented reality;Collaborative learning;Scheduling policies},
doi={10.1109/IPDPSW.2018.00063},
ISSN={null},
month={May},}
@INPROCEEDINGS{7516324,
author={M. F. {Norraji} and M. S. {Sunar}},
booktitle={2015 4th International Conference on Interactive Digital Media (ICIDM)},
title={wARna — Mobile-based augmented reality colouring book},
year={2015},
volume={},
number={},
pages={1-4},
abstract={Usage of Augmented Reality (AR) in educational environment is not an alien thing recently, especially when considering the multimodality and interactivity nature of some AR application that proves to be a lot more immersive and engaging learning than its traditional counterpart. Regardless, another aspect of educational AR, or any AR app, that should not be overlooked is the robustness as well as the availability of the app itself. In this paper, we are discussing a type of mixed-reality book experience that augments a colouring book with user-manipulated three-dimensional (3D) contents in a mobile-based environment. We explore tracking approaches in AR for its efficiency as well as computational load that is most suitable for mobile environment, and we also investigate texture extraction and mapping method for 3D content. Our system is then based on a marker-based tracking system using fiducial marker and simple image processing technique for the texture extraction and mapping part.},
keywords={augmented reality;computer aided instruction;feature extraction;image texture;mobile learning;wARna;mobile-based augmented reality colouring book augments;educational environment;multimodality interactivity;immersive application;learning engagement;mixed-reality book experience;user-manipulated three-dimensional contents;user-manipulated 3D contents;computational load;efficiency analysis;texture extraction;mapping method;3D content;marker-based tracking system;fiducial marker;image processing technique;Three-dimensional displays;Augmented reality;Mobile communication;Image color analysis;Feature extraction;Libraries;Augmented Reality;Marker-based Detection;AR in Education;Texture Extraction},
doi={10.1109/IDM.2015.7516324},
ISSN={null},
month={Dec},}
@INPROCEEDINGS{7349981,
author={A. S. Y. {Lai} and C. Y. K. {Wong} and O. C. H. {Lo}},
booktitle={2015 IEEE 12th International Conference on e-Business Engineering},
title={Applying Augmented Reality Technology to Book Publication Business},
year={2015},
volume={},
number={},
pages={281-286},
abstract={Augmented Reality is an emerging technology and the applications of technology are still not fully unveiled. This paper explores a new application of augmented reality for a new direction in educational book publishing, which aims to bring interactive learning experience to life. The project takes printed images on book to the next level by applying Augmented Reality technology to provide a unique fascinating experience to its readers on mobile devices. Augmented Reality (AR) technology composing with animation brings new digital entertainment experience to the reader of books. The key feature of this paper uses the technology presents auxiliary information in the field of view of an object printed on book automatically without human intervention. The project uses the technology with iPad mobile device to display 3D models, 3D animations, video splaying, websites and web server connectivity for children education. The results and evaluation of the project shows the interactive 3D animation and self-assessment functions significantly support students to improve their learning experience and performance. The software product of this project, from the business perspective, creates a new business marketing dimension in digital publishing and increases the selling profits in the book publication business.},
keywords={augmented reality;computer aided instruction;computer animation;electronic commerce;interactive systems;marketing;publishing;augmented reality technology;book publication business;educational book publishing;interactive learning;AR technology;digital entertainment;iPad mobile device;interactive 3D animation;business marketing dimension;digital publishing;Three-dimensional displays;Augmented reality;Engines;Animation;Target tracking;Databases;Solid modeling;Augmented Reality;Mobile Computing;Multimedia Services;E-Learning},
doi={10.1109/ICEBE.2015.55},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8252384,
author={R. {Krstulovic} and I. {Boticki} and H. {Ogata}},
booktitle={2017 IEEE 6th International Conference on Teaching, Assessment, and Learning for Engineering (TALE)},
title={Analyzing heterogeneous learning logs using the iterative convergence method},
year={2017},
volume={},
number={},
pages={482-485},
abstract={This paper presents the use of iterative convergence method in analyzing learning log data from a three year mobile learning project. The authors of the paper propose the use of iterative convergence method as a non-standard student' evaluation method based on lessons' weights. During the project duration a large amount of log data on competitive, collaborative and augmented reality digital lesson use was collected and stored in a proprietary database. The method calculates lessons' weights and students' success in a scenario where there are no prior lessons' weights or students' success known, and where not all students complete the same set of lessons. After the application of the algorithm, the students' success data on the three different types of lessons is compared and correlated. The main implications coming for the analysis is that students have better success in competitive and augmented reality lessons and that there is negative correlation between students' success on competitive and augmented reality lessons on one side and collaborative lessons on another.},
keywords={augmented reality;computer aided instruction;convergence of numerical methods;data analysis;groupware;iterative methods;mobile learning;collaborative lessons;iterative convergence method;nonstandard student evaluation method;competitive reality digital lesson;augmented reality digital lesson;lessons weights;mobile learning project;learning log data analysis;Iterative methods;Convergence;Collaboration;Augmented reality;Correlation;Mobile communication;mobile learning;learning log analysis;iterative convergence method},
doi={10.1109/TALE.2017.8252384},
ISSN={2470-6698},
month={Dec},}
@INPROCEEDINGS{8005387,
author={L. {Lee} and C. {Chau} and C. {Chau} and C. {Ng}},
booktitle={2017 International Symposium on Educational Technology (ISET)},
title={Using Augmented Reality to Teach Kindergarten Students English Vocabulary},
year={2017},
volume={},
number={},
pages={53-57},
abstract={Augmented Reality (AR) is a technology that augments the real physical world with computer-generated 3D virtual objects such that the users can interact with them using the screen of their mobile devices. This paper studies how to effectively use AR to enhance the learning experience of kindergarten students, while addressing parents' concern that a long-time usage of electronic devices may affect their child's health. We developed an AR mobile application prototype to teach kindergarten students English vocabulary in an interactive and attractive way. It allows kindergarten students to learn English vocabulary in any place and at any time using a mobile device. To address the parents' concern on health, we integrate a monitoring system into the application, which allows the parents to monitor their child's usage and stop the application in real time online. Preliminary evaluation shows that the effectiveness of the application is satisfactory. It is beneficial to use augmented reality for early childhood education if the usage time of the students is well monitored.},
keywords={augmented reality;computer aided instruction;human computer interaction;mobile computing;teaching;vocabulary;augmented reality;kindergarten student;English vocabulary teaching;computer-generated 3D virtual objects;mobile devices;learning experience;electronic devices;AR mobile application prototype;monitoring system;early childhood education;Vocabulary;Mobile handsets;Prototypes;Pediatrics;Monitoring;Games;Education;augmented reality;English learning;early childhood education;educational games;parental perspectives},
doi={10.1109/ISET.2017.20},
ISSN={null},
month={June},}
@INPROCEEDINGS{8280626,
author={S. {Moedjiono} and A. {Kusdaryono} and {Nurcahyadi}},
booktitle={2017 Second International Conference on Informatics and Computing (ICIC)},
title={Media Interactive Learning and biology subjects implementation with augmented reality application},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Media Interactive Learning will be very beautiful if applied in the world of education, especially high school students that on average have Smartphones. Therefore, can be designed a media learning application that will use the biology subjects with the material in human organs, which will use the materials of the high school grade XI. Android is a container that can be used to apply media applications learning, using Augmented Reality (AR) technology which is often called AR Media learning would be more interesting to use. The result is media learning and biology subjects with AR technology using a marker Natural Feature Tracking reading techniques.},
keywords={augmented reality;biology computing;computer aided instruction;mobile learning;biology subjects;augmented reality application;media applications learning;media interactive learning;Android;AR media learning;natural feature tracking reading techniques;Media;Biology;Smart phones;Androids;Humanoid robots;Augmented reality;Education;Augmented Reality (AR);smartphone;marker;Natural Feature Tracking;Android},
doi={10.1109/IAC.2017.8280626},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8783620,
author={L. F. {García Arias} and N. D. {Duque Méndez} and C. {Dias Flores}},
booktitle={2018 XIII Latin American Conference on Learning Technologies (LACLO)},
title={Augmented Reality Learning Resources in Anatomy},
year={2018},
volume={},
number={},
pages={476-483},
abstract={Augmented reality can be applied in different areas of knowledge and one of the most explored fields has been health. The objective of this article is to present the results obtained from the evaluation of learning resources developed using augmented reality technology. The evaluation was made by students of health undergraduate programs who participated in the Academic Day of the Biomedicine program of a Brazilian higher education institution. 8 educational resources were evaluated, all related to the area of general anatomy. The resources were evaluated by 41 students who answered a validated questionnaire for the evaluation of educational resources with questions about: learning, interactivity, engagement, attractiveness, functionality and autonomy. The evaluation was considered valid. The challenge is to find interactive alternatives that stimulate and simultaneously incorporate content, with a depth appropriate to the objective of the subject. Critics in the evaluation will serve as the basis for adjustments in the next learning resources to be developed.},
keywords={augmented reality;biomedical education;computer aided instruction;educational institutions;further education;Brazilian higher education institution;augmented reality learning resources;health undergraduate programs;biomedicine program;Three-dimensional displays;Visualization;Augmented reality;Education;Solid modeling;Software;Two dimensional displays;Augmented reality;general anatomy;learning resource},
doi={10.1109/LACLO.2018.00085},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{7474634,
author={H. {Al-Ali} and M. W. {Bazzaza} and M. J. {Zemerly} and J. W. P. {Ng}},
booktitle={2016 IEEE Global Engineering Education Conference (EDUCON)},
title={MyVision AIR: An augmented interactive reality book mobile application},
year={2016},
volume={},
number={},
pages={741-745},
abstract={This paper presents an Augmented Interactive Reality book (AIR) application, aimed to enhance the reading experience of adult-learners by incorporating Augmented Reality (AR) technology to improve the interaction of normal books. Various features and characteristics of Augmented Reality (AR) were applied to the chosen book, "My Vision" written by H.H. Sheikh Mohammed Bin Rashid Al Maktoum, Vice-President and Prime Minister of the UAE and Ruler of Dubai. The project is a joint-collaboration between Etisalat British Telecommunication Innovation Center (EBTIC) and Khalifa University, as part of the iCampus project on "Smart Learning-Edutainment". The project has received a number of positive feedbacks from the field study conducted with the general public, and has also been presented to H.H. Sheikh Mohammed Bin Rashid Al Maktoum - Ruler of Dubai, H.H. Sheikh Saif Bin Zayed Al Nahyan - Deputy Prime Minister and Minister of Interior, and several others.},
keywords={augmented reality;computer science education;MyVision AIR;augmented interactive reality book mobile application;AIR application;Etisalat British Telecommunication Innovation Center;EBTIC;Khalifa University;iCampus project;Smart Learning-Edutainment;Augmented reality;Three-dimensional displays;Solid modeling;Cameras;Software;Animation;Creativity},
doi={10.1109/EDUCON.2016.7474634},
ISSN={2165-9567},
month={April},}
@INPROCEEDINGS{7474591,
author={M. W. {Bazzaza} and M. {Alzubaidi} and M. J. {Zemerly} and L. {Weruga} and J. {Ng}},
booktitle={2016 IEEE Global Engineering Education Conference (EDUCON)},
title={Impact of smart immersive mobile learning in language literacy education},
year={2016},
volume={},
number={},
pages={443-447},
abstract={With the fast paced improvements in smart devices such as tablets and smartphones, many schools in the developed countries are trying to improve the traditional pedagogical methods by harnessing smart learning technology for different purposes. In this paper, we proposed the use of a smart novel educational technology to immerse the students in the learning process by engaging as many of the students' senses as possible through the creation of interactive elements and entertaining features. This process is also called edutainment, combining both education and entertainment, and it works by making students learn subconsciously in an immersive manner. The aim of this paper is to demonstrate how an immersive augmented reality application can work in conjunction with Arabic schoolbooks in an attempt to improve Arabic literacy for children in the UAE. The reason for this is that children in the UAE are being exposed to English most of the time and, as a consequence, gradually losing their mother tongue. A focus field-study was then conducted on two groups of Grade 1 students to get a pre-understanding in the impact of such educational technology, targeted for younger learners, in language literacy education.},
keywords={augmented reality;computer aided instruction;educational institutions;entertainment;mobile learning;smart phones;smart immersive mobile learning;language literacy education;smart devices;smartphones;pedagogical methods;edutainment;entertainment;immersive augmented reality;Arabic schoolbooks;Arabic literacy;UAE;English;mother tongue;Augmented reality;Three-dimensional displays;Education;Animation;Solid modeling;Smart phones;Immersive Augmented Reality;Smart Education;Edutainment;Augmented Reality;Game-Based Learning},
doi={10.1109/EDUCON.2016.7474591},
ISSN={2165-9567},
month={April},}
@INPROCEEDINGS{8257153,
author={C. N. {Qamari} and M. R. {Ridwan}},
booktitle={2017 3rd International Conference on Science in Information Technology (ICSITech)},
title={Implementation of Android-based augmented reality as learning and teaching media of dicotyledonous plants learning materials in biology subject},
year={2017},
volume={},
number={},
pages={441-446},
abstract={The field of education so far often make use of technology such as presentation slides and also interactive programs as a learning and teaching media. This study attempted to apply the Android-based augmented reality (AR) technology to display the learning materials in the digital format so the students can observe the overall learning object with the help of Android application. The main aim of this study was to understand the level of interest and inputs from the students related to the provided learning object, so it can be developed better in the next chance. This study was done using the qualitative approach method with the results sourced from the observation and simulation research method, using questionnaire form given to 24 students where the questions were based from the direct simulation using Android smartphone. The results show that 85,4% of the overall students are very interested in using AR media, 95,8% stated strongly agree that the biology learning material becomes more easily understood using AR media, 86,5% stated that AR media is very easy and practical to use, 76% stated strongly agree that their Android smartphone can function well to use AR media, and 65,6% stated that the AR cards used in this study can be scanned easily with using the application. Overall, the average percentage of all students' opinions was 81,9%, which was interpreted as a strongly agreed option that AR media is interesting to be applied as a media for studying dicotyledonous plants in the biology subject.},
keywords={augmented reality;computer aided instruction;smart phones;teaching;augmented reality;dicotyledonous plants;biology subject;learning materials;Android application;qualitative approach method;simulation research method;Android smartphone;AR media;biology learning material;learning object;Media;Three-dimensional displays;Education;Biology;Augmented reality;Androids;Humanoid robots;android;augmented reality;interest;learning and teaching media},
doi={10.1109/ICSITech.2017.8257153},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8569170,
author={P. {Wang} and T. {Hsu}},
booktitle={2018 1st IEEE International Conference on Knowledge Innovation and Invention (ICKII)},
title={Application of amplified reality to the cognitive effect of children with attention deficit hyperactivity disorder(ADHD) – An example of Italian Chicco-app interactive building blocks},
year={2018},
volume={},
number={},
pages={301-302},
abstract={For children with attention deficit hyperactivity disorder (ADHD), most are mixed symptoms, such as Asperger syndrome (AS), Tourette's syndrome (TS), Anxiety disorders, and so on, just collectively known as ADHD, but the most obvious symptoms are inattention, learning disabilities, emotional disorders, sensory integration abnormalities . Therefore, this research focuses on building blocks and Augmented Reality (AR) links, using the building blocks of animals as the theme, coupled with the operation of building blocks to enhance patience, touch, construction and other capabilities, through the interaction of AR and dynamic performance can attract attention to ADHD children's eyes and attention . There are two objectives in this study: Firstly, is to have a positive interaction with AR for children under six years of age who have ADHD . Secondly is to increase the difficulty of interaction, improve the continuous response to designed AR and attract; Not only use the building blocks to interact, but also through the AR increase in children's patience, initiative, associated with the combination of AR and tablet. From this trend, it can increase the child's touch operation action. Finally, through interviews, experimental teaching, observation and evaluation, this study explores the effects of APP interactive blocks on the learning of ADHD children.},
keywords={augmented reality;cognition;medical disorders;mobile computing;paediatrics;psychology;user interfaces;Asperger syndrome;Tourette's syndrome;emotional disorders;APP interactive blocks;ADHD children;amplified reality;Italian Chicco-app interactive building blocks;augmented reality links;sensory integration abnormalities;learning disabilities;inattention;cognitive effect;attention deficit hyperactivity disorder;anxiety disorders;Animals;Education;Technological innovation;Augmented reality;Pediatrics;Conferences;Interviews;ADHD Children;Augmented Reality;Interactivity;Building Blocks},
doi={10.1109/ICKII.2018.8569170},
ISSN={null},
month={July},}
@INPROCEEDINGS{8376921,
author={F. Y. {Al-Hammadi} and A. F. {Aldarwish} and A. H. {Alasmakh} and M. J. {Zemerly}},
booktitle={2018 Advances in Science and Engineering Technology International Conferences (ASET)},
title={Augmented reality in educational games: City of Life (COL) emirati sustainability-edutainment interactive game},
year={2018},
volume={},
number={},
pages={1-7},
abstract={Learning about sustainability Is not just a simple matter of memorizing facts. Sustainability education necessitates the employment of features that inspire discussion and develop decision-making skills. Video games have a huge potential to be fully functional learning environments for learning about sustainability. City of Life is an interactive mobile game application that teaches students at various levels of study about some sustainability aspects, which encompass the issues and tasks for achieving the sustainable development goals identified by the United Nation (UN). Its main theme is city building, where the player will be able to build their very own unique sustainable city using eye-catching, realistic, UAE-culture relevant buildings; further, it employs educational parts using attractive stylish mechanics such as Augmented Reality, with some mini-games to entertain and verify the player's educational knowledge.},
keywords={augmented reality;computer aided instruction;computer games;history;mobile learning;sustainable development;teaching;educational games;sustainability education;decision-making skills;video games;interactive mobile game application;sustainability aspects;sustainable development goals;city building;unique sustainable city;UAE-culture relevant buildings;Augmented Reality;Augmented reality;Games;Sustainable development;Urban areas;Education;Buildings;Augmented reality;Tools;City Building;Edutainment;Sustainability;Education;Augmented Reality;Mobile game application},
doi={10.1109/ICASET.2018.8376921},
ISSN={null},
month={Feb},}
@INPROCEEDINGS{8874917,
author={A. K. {Wahyudi} and J. {Yuan Mambu} and A. V. {Lengkong} and {Nurhadi} and Z. {Ardian}},
booktitle={2019 1st International Conference on Cybernetics and Intelligent System (ICORIS)},
title={Implementing Augmented Reality as a Digital Props of Brain Anatomy using 3D Cuboid Tracker},
year={2019},
volume={1},
number={},
pages={151-155},
abstract={Augmented Reality may propose several advantages over the conventional method as a teaching aid, including in learning brain anatomy in health education. To gives an alternative to the typical props, and we proposed a learning aid in the form of AR mobile application where a student can hold and observe the human brain in the form of a 3D digital props. By utilizing the 3D cuboid tracker, the proposed system offers an affordable digital prop and yet allow natural handling of the object, which gives a new learning experience. Through performance testing, we found how the device specifications' may slightly affect the proposed system performance. However, through a usability testing, the proposed system and method is preferred over other methods such as a book, information on the screen, and 2D AR object. This research shows the proposed system and method have a lot of potential in aiding much more interactive and stimulating learning activities.},
keywords={augmented reality;biomedical education;brain;computer aided instruction;ergonomics;health care;interactive systems;mobile learning;teaching;interactive learning activities;augmented reality;digital props;brain anatomy;3D cuboid tracker;health education;learning aid;AR mobile application;human brain;performance testing;system performance;usability testing;Three-dimensional displays;Smart phones;Cameras;Two dimensional displays;Testing;Education;Usability;augmented reality;brain anatomy;mobile application;cuboid tracker;health education},
doi={10.1109/ICORIS.2019.8874917},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{8653666,
author={N. M. {Kumar} and P. R. {Krishna} and P. K. {Pagadala} and N. M. {Saravana Kumar}},
booktitle={2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on},
title={Use of Smart Glasses in Education-A Study},
year={2018},
volume={},
number={},
pages={56-59},
abstract={Smart glasses proven to be one of the modern computing devices that unite the humans and machines with the help of information and communication technology (ICT). In recent years, it is seen that smart glasses have been used in the medical and gaming applications. However, the features of smart glasses can contribute its services in other fields too. In this paper, a study is carried out to explore the possible application of smart glasses in the education sector. In the investigation, most features of smart glass were found to be in favours with the requirements of teaching and learning process adopted in the education sector. Typical applications of wearable smart glass in education include the augmented reality, documentation of lecture, on-site report preparation, recording lectures as videos, capturing essential points as images, telementoring, trainee's evaluation, understanding the listener's experience and nature, student concentration evaluation etc. Besides, these the possible benefits of adopting and challenges in implementing are also explored. The outcome of this study suggests that the implementation of smart glasses in the education sector will enhance the concept of ICT education.},
keywords={computer aided instruction;interactive devices;teaching;education sector;wearable smart glass;information and communication technology;ICT education;teaching process;learning process;Smart glasses;Education;Videos;Augmented reality;Head;Optical devices;Smart glass;eyewear devices;HMD’s;head-mounted displays;OHMD’s;wearable technology;smart glass in education;application of smart glass in education;digital education;google glass},
doi={10.1109/I-SMAC.2018.8653666},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{7386050,
author={J. K. T. {Tang} and T. A. {Duong} and Y. {Ng} and H. {Luk}},
booktitle={2015 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)},
title={Learning to create 3D models via an augmented reality smartphone interface},
year={2015},
volume={},
number={},
pages={236-241},
abstract={The ordinary way of creating 3D models (a.k.a. 3D modeling) requires people to sit in front of the computer for a long time working with professional software. The license fees of these software are expensive, and it is very time consuming and not easy to learn. Teachers sometimes find difficult to teach students the concept of 3D modeling because the newbies need to spend a lot of time to familiarize with the tool interface beforehand. In this paper, we introduce a mobile application that assists students to learn 3D modeling skills and concepts. We provide a natural modeling style with the aid of Augmented Reality (AR). Through a smartphone user interface, the user builds a model with primitive blocks in a "bottom-up" manner like "LEGO" bricks. These blocks are visualized on the printed marker cards that allow users to manipulate (rotate, translate, etc.) them in the same way of manipulating real building blocks. User studies have been conducted and we have identified some aspects that help people to model 3D objects.},
keywords={augmented reality;computer aided instruction;smart phones;user interfaces;create 3D models;augmented reality smartphone interface;professional software;license fees;tool interface;mobile application;3D modeling skills;3D modeling concepts;AR;smartphone user interface;printed marker cards;Three-dimensional displays;Solid modeling;Shape;Computational modeling;Augmented reality;Education;Mobile applications;Augmented Reality;3D Modeling;Mobile Learning;Interactive Learning;Smart Education},
doi={10.1109/TALE.2015.7386050},
ISSN={null},
month={Dec},}
@INPROCEEDINGS{8674350,
author={E. R. {Nainggolan} and H. H. {Asymar} and A. R. A. {Nalendra} and {Anton} and F. {Sulaeman} and {Sidik} and U. {Radiyah} and {Susafa’ati}},
booktitle={2018 6th International Conference on Cyber and IT Service Management (CITSM)},
title={The Implementation of Augmented Reality as Learning Media in Introducing Animals for Early Childhood Education},
year={2018},
volume={},
number={},
pages={1-6},
abstract={The early childhood education is the process of learning where the children are on the step to know and to be curious of their surroundings. It is also a time for children to grow and to explore the knowledge. Nowadays some schools of early childhood still use the old method such as of face-to-face or read book. Consequently, it makes the students of early childhood easily bored with this method and need the new method to make they are interested in. Hence, it needs an interactive learning media to make the children interest in. The selection of making media of introduction animal based on recognition of Augmented reality using Unity 3D because it supports Animated Augmented reality itself which contained by animations, the text, images, 3D, audio, and video as well as Vuforia mobile device SDK that enables the creation of Augmented reality. The FAST Corner Detection algorithm is used in this system for the purpose of speeding up computing time in real-time with the consequences of lowering the level of accuracy of detection angle. The Aim of this research is to give the detail information about the using of visual information about the animals interactively through the three-dimensional animation using the technology of Augmented Reality. The result of this research is the application of interactively learning media based on the augmented reality. It can help the teacher of the early childhood education (PAUD) to introduce the name, shape even sound of the animal to the pupils.},
keywords={augmented reality;computer aided instruction;computer animation;mobile computing;user interfaces;animated augmented reality;FAST corner detection algorithm;Vuforia mobile device SDK;PAUD;three-dimensional animation;introduction animal;children interest;interactive learning media;face-to-face;old method;early childhood education;Augmented reality;Education;Media;Animals;Games;Real-time systems;Information technology;Augmented reality;The introduction of animals;Early Childhood Education Program;Algorithm Fast Corner Detection},
doi={10.1109/CITSM.2018.8674350},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{7756984,
author={M. B. {Ibáñez} and Á. D. {Serio} and D. {Villarán} and C. {Delgado-Kloos}},
booktitle={2016 IEEE 16th International Conference on Advanced Learning Technologies (ICALT)},
title={The Acceptance of Learning Augmented Reality Environments: A Case Study},
year={2016},
volume={},
number={},
pages={307-311},
abstract={The aim of this study was to investigate the attitude of learners toward an augmented reality learning activity designed to help engineering students to solve an electromagnetic problem. The sample was 122 students. Students were asked to complete a survey questionnaire based on the Technology Acceptance Model (TAM) enhanced with perceived enjoyment items. The results of the evaluation show that intention of use the system is dependent of perceived enjoyment but not from perceived usefulness of the learning tool.},
keywords={augmented reality;computer aided instruction;electromagnetism;engineering computing;engineering education;human factors;learning tool;perceived enjoyment;TAM;technology acceptance model;electromagnetic problem;engineering students;augmented reality learning activity;learner attitude;learning augmented reality environment;Conferences;Augmented reality;Augmented reality;assessment;interactive learning environments;technology acceptance model},
doi={10.1109/ICALT.2016.124},
ISSN={2161-377X},
month={July},}
@INPROCEEDINGS{8970136,
author={M. C. {Costa} and A. {Manso} and J. M. {Patrício} and A. {Carvalho} and B. {Alegria} and V. {Zinatulins}},
booktitle={2019 International Symposium on Computers in Education (SIIE)},
title={An augmented reality platform targeted to promote learning about planetary systems},
year={2019},
volume={},
number={},
pages={1-5},
abstract={We present a mobile augmented reality platform targeted to promote learning about planetary systems. The architecture of this platform includes a mobile application and a back-office that allows teachers to choose the planetary system they intend to present to their students. Furthermore, they can introduce information about celestial bodies, such as stars or planets and develop a set of multiple-choice questions to assess student's learning about the subject matters they teach. Also, after playing the game, the data collected by the application is sent to the information system that processes it and makes it available to teachers. With these functionalities, this paper intends to propose this platform as a resource to be implemented in any level of school syllabus.},
keywords={Mobile learning;augmented reality;collaborative learning environments;learning object repositories;astronomy},
doi={10.1109/SIIE48397.2019.8970136},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8567219,
author={I. {Chen}},
booktitle={2018 1st International Cognitive Cities Conference (IC3)},
title={The Application of Augmented Reality in English Phonics Learning Performance of ESL Young Learners},
year={2018},
volume={},
number={},
pages={255-259},
abstract={Phonics is an essential foundation needed to build up reading and writing abilities for English learning. However, most students who learned English as a second language have not learned phonics systematically and appropriately, resulting in failed pronunciation in reading and writing English words. For eliminating struggles and barriers in the process of English learning, the new technology and interactive apps are a dynamic avenue assisting with understanding and retention for ESL elementary students. The study was conducted with augmented reality (AR) technology to integrate virtual objects and video clips into the interactive learning environment for second language learning. The effects of students' phonics learning performance were assessed. The results indicated that AR technology had validated the possibility of carrying out digital immersive language learning and embodied cognition, regardless of the drills of rote memorization. The concerns of the curriculum design based on the incorporation of the AR technology and learning materials are discussed further to improve phonics efficiency and competency for English as second language learners.},
keywords={augmented reality;computer aided instruction;ESL young learners;writing abilities;English learning;ESL elementary students;augmented reality;interactive learning environment;AR technology;digital immersive language learning;learning materials;phonics efficiency;language learners;English phonics learning performance;Games;Education;Augmented reality;Vocabulary;Writing;Bridges;Visualization;augmented reality, phonics learning, ESL},
doi={10.1109/IC3.2018.000-7},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{8864173,
author={S. {Sittiyuno} and K. {Chaipah}},
booktitle={2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
title={ARCode: Augmented Reality Application for Learning Elementary Computer Programming},
year={2019},
volume={},
number={},
pages={32-37},
abstract={This paper presented ARCode, the system that employed augmented reality (AR) to motivate and help learners with programming. Since programming is an abstract task, many need help with programming, both technically and emotionally. ARCode is the game-based learning system that shows how each command works by using AR animations, and focuses on the logical order of commands. Learners can collaborate in the real world while practicing individually in the application. In our experiment, we found that the treatment group had significantly improved on their scores for all topics except for Selection, while the control group did not. The user satisfaction survey suggested that more than 80% of users accepted our system as a useful, enjoyable, and collaborative learning system. The primary results showed the system's potential to help learners better learn programming with high motivation.},
keywords={augmented reality;computer aided instruction;computer games;computer science education;useful learning system;collaborative learning system;ARCode;augmented reality application;elementary computer programming;abstract task;command works;treatment group;control group;user satisfaction survey;Programming;Tools;Animation;Games;Visualization;C++ languages;History;Learning Programming;Augmented Reality;Mobile Learning;Game-based Learning},
doi={10.1109/JCSSE.2019.8864173},
ISSN={2372-1642},
month={July},}
@INPROCEEDINGS{8586762,
author={M. T. {Mahmoudi} and F. Z. {Zeraati} and P. {Yassini}},
booktitle={2018 12th Iranian and 6th International Conference on e-Learning and e-Teaching (ICeLeT)},
title={A Color Sensing AR-Based Interactive Learning System for Kids},
year={2018},
volume={},
number={},
pages={013-020},
abstract={In the last decade, there has been a rapid growth in applying Augmented Reality (AR) and Internet of Things (IoT) as emerging technologies in linking physical and virtual objects for educational purposes. Realizing the complex concepts in a way that learner be able to interact with, will not only elevate the learning passion, but also has a significant positive role on learning performance and student's engagement. According to the aforementioned points, in this paper, we propose a color sensing AR-Based interactive learning system for kids which helps them to scan and explore the colors and learn their corresponding words in Spanish language. Our aim is to explore the role of such an AR-IoT-based system on students' learning performance. To do this, an embedded system including a color sensor and a raspberry pi board was implemented and provided for the kids. By placing the sensor on the specially designed colored book, the light's color frequency is measured and RGB code is sent over the cloud using MQTT protocol. On the web application side, color data is fetched from the cloud server and learners will be provided with a real-time feedback with the corresponding animations and multimedia content that teaches colors in Spanish language. Students' learning performance was assessed using paired t-test based upon the results of their pre-test and post-test, before and after interacting with the system. The successful experimental results were achieved proving the effectiveness of the system on learning performance.},
keywords={augmented reality;computer aided instruction;computer animation;interactive systems;Internet of Things;teaching;color sensing AR-based interactive learning system;kids;learning passion;AR-IoT-based system;students;color sensor;color data;learning performance;colored book;color frequency;Augmented Reality;Internet of Things;IoT;Image color analysis;Internet of Things;Embedded systems;Cloud computing;Servers;Animation;Frequency measurement;Augmented reality;internet of things;color sensing;interactive learning},
doi={10.1109/ICELET.2018.8586762},
ISSN={2163-6982},
month={March},}
@INPROCEEDINGS{8450974,
author={M. G. {Gramajo} and F. {Trejo Lezcano} and S. G. {Lobo} and G. {Juarez} and A. L. {Fraga}},
booktitle={2018 IEEE World Engineering Education Conference (EDUNINE)},
title={SIMNET: Simulation-Based Exercises for Computer Network Curriculum Through Gamification and Augmented Reality},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Gamification and Augmented Reality techniques, in recent years, have tackled many subjects and environments. Its implementation can, in particular, strengthen teaching and learning processes in schools and universities. Therefore, new forms of knowledge, based on interactions with objects, contributing game, experimentation and collaborative work. Through the technologies mentioned above, we intend to develop an application that serves as a didactic tool, giving support in the area of Computer Networks. This tool aims to stand out in simulated controlled environments to create computer networks, taking into account the necessary physical devices and the different physical and logical topologies. The main goal is to enrich the students' learning experiences and contribute to teacher-student interaction, through collaborative learning provided by the tool, minimizing the need for expensive equipment in learning environments.},
keywords={augmented reality;computer aided instruction;computer games;computer networks;computer science education;educational institutions;groupware;teaching;computer network curriculum;gamification;Augmented Reality techniques;teaching;learning processes;schools;universities;collaborative work;didactic tool;computer networks;teacher-student interaction;collaborative learning;learning environments;SIMNET;simulation-based exercises;Tools;Computer networks;Games;Education;Androids;Humanoid robots;Augmented reality;Gamijication;Augmented Reality;Simulation-based exercises},
doi={10.1109/EDUNINE.2018.8450974},
ISSN={null},
month={March},}
@INPROCEEDINGS{8001839,
author={T. {Bratitsis} and P. {Bardanika} and M. {Ioannou}},
booktitle={2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)},
title={Science Education and Augmented Reality Content: The Case of the Water Circle},
year={2017},
volume={},
number={},
pages={485-489},
abstract={Augmented Reality (AR) is a live, direct or indirect projection of the physical-real world which allows users to experience the surrounding environment as it is, in real-time, enhanced with digital and/or interactive content. It seems that AR can be exploited for teaching various disciplines. In this paper, the preliminary results of a study regarding a teaching intervention about science education in primary school and specifically the topic of the water circle are presented. The AR content was deployed through the ENTITI creator application.},
keywords={augmented reality;computer aided instruction;science education;augmented reality content;Water Circle;AR;teaching intervention;ENTITI creator application;Education;Water conservation;Augmented reality;Mobile handsets;Multimedia communication;Rivers;Earth;augmented reality;primary school;water circle;ENTITI creator;science education},
doi={10.1109/ICALT.2017.64},
ISSN={2161-377X},
month={July},}
@INPROCEEDINGS{7441163,
author={J. A. {Frank} and V. {Kapila}},
booktitle={2016 Indian Control Conference (ICC)},
title={Towards teleoperation-based interactive learning of robot kinematics using a mobile augmented reality interface on a tablet},
year={2016},
volume={},
number={},
pages={385-392},
abstract={The integration of augmented reality (AR) techniques in user interface design has enhanced interactive experiences in teleoperation of robots, hands-on learning in classrooms, laboratory, and special education, and user training in an array of fields, e.g., aerospace, automotive, construction, manufacturing, medical, etc. However, AR-based user interfaces that command machines and tools have not been fully explored for their potential to enhance interactive learning of engineering concepts in the laboratory. This paper outlines the development of a mobile application executing on a tablet device, which renders an immersive AR-based graphical user interface to enable users to monitor, interact with, and control a four-link underactuated planar robot. Computer vision routines are used to extract real-time, vision-based measurements of the robot's joint angles and end effector location from the live video captured by the rear-facing camera on the tablet. The obtained measurements are used to render AR content to offer users with additional visual feedback. Touch gesture recognition is implemented to allow users to naturally and intuitively command the robot by tapping and dragging their fingers at desired locations on the tablet screen. Experimental results show the performance and efficacy of the proposed system as it is operated in two different modes: one in which the user has direct control over the angles of the actuated links of the robot and one in which the user has direct control over the end effector location.},
keywords={angular measurement;augmented reality;computer aided instruction;computer vision;control engineering education;end effectors;gesture recognition;graphical user interfaces;manipulator kinematics;mobile computing;notebook computers;rendering (computer graphics);telerobotics;touch sensitive screens;teleoperation-based interactive learning;mobile augmented reality interface;interactive experiences;hands-on learning;classrooms;laboratory;special education;user training;engineering concepts;mobile application;tablet device;immersive AR-based graphical user interface design;four-link underactuated planar robot control;computer vision routines;real-time vision-based measurements;robot joint angles;end effector location;live video;rear-facing camera;AR content;visual feedback;touch gesture recognition;Kinematics;Robot kinematics;End effectors;User interfaces;DC motors;Mathematical model},
doi={10.1109/INDIANCC.2016.7441163},
ISSN={null},
month={Jan},}
@INPROCEEDINGS{7372343,
author={M. {Sugimoto}},
booktitle={2015 International Conference on Computer Application Technologies},
title={Augmented Tangibility Surgical Navigation Using Spatial Interactive 3-D Hologram zSpace with OsiriX and Bio-Texture 3-D Organ Modeling},
year={2015},
volume={},
number={},
pages={189-194},
abstract={We developed a new spatial navigation system for medical informatics by interactive superimposing 3-D hologram and 3D printing technology. Interactive stereo display was used for the existence of an interaction between the users and stereo images of the patient's anatomy depicted on the display in the form of tracking the user's head and hand/arm position. Sensing the user's head position created motion parallax information, an immersive depth cue that can be added to the binocular parallax already present in the display. We also developed new technology of bio-texture modeling by multi-material 3D printing to form 3D organ textures and structures. Based on patient-specific MDCT data sets, regions of interest were segmented using DICOM viewer OsiriX application. After generating 3D surface models of the organ and STL file out of the patient's 3D data, the inkjet 3D printer created a 3D multi-material organ replica. Sensing the user's hand or arm position using motion sensor attached the patient's life size 3-D printed organ model, allowed the user to manipulate the spatial attributes of the virtual and real printed organs, which can enhance spatial reasoning and augmented tangibility. These tangible organ replication provide better anatomical reference tool as a tailormade simulation and navigation, and contribute to medical safety and accuracy, less-invasiveness and improvement of the medical education for students and trainees.},
keywords={augmented reality;biological organs;computerised tomography;holography;image motion analysis;image segmentation;image texture;ink jet printing;medical image processing;stereo image processing;surgery;three-dimensional printing;STL file;inkjet 3D printer;3D multimaterial organ replica;arm position;motion sensor;patient's life size 3D printed organ model;3D surface models;segmented DICOM viewer OsiriX application;regions-of-interest;patient-specific MDCT data sets;3D organ textures;multimaterial 3D printing;bio-texture modeling;binocular parallax;immersive depth cue;motion parallax information;user's head position;hand-arm position;user's head tracking;patient anatomy;stereo images;interactive stereo display;3D printing technology;interactive superimposing 3-D hologram;medical informatics;spatial navigation system;Bio-Texture 3D organ modeling;OsiriX 3D organ modeling;spatial interactive 3D hologram zSpace;augmented tangibility surgical navigation;spatial attributes;real printed organs;virtual printed organs;spatial reasoning;augmented tangibility;tangible organ replication;anatomical reference tool;tailor-made simulation;tailor-made navigation;medical safety;medical accuracy;medical education;Three-dimensional displays;Surgery;Biological systems;Solid modeling;Navigation;Medical diagnostic imaging;Virtual reality;Augmented reality;Mixed reality;3-D printer;OsiriX},
doi={10.1109/CCATS.2015.53},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{7504776,
author={X. {Wei} and D. {Weng} and Y. {Liu} and Y. {Wang}},
booktitle={2016 IEEE Virtual Reality (VR)},
title={A tour guiding system of historical relics based on augmented reality},
year={2016},
volume={},
number={},
pages={307-308},
abstract={Yuanmingyuan is a relic park and only few cultural relics are left due to the looting and burning down in history, which makes that most of the scenic spots of the park look boring. To address such issue, a game-based guidance system for Yuanmingyuan and a time travel game called MAGIC-EYES has been proposed with Augmented Reality technology. Six interactive modes are designed in the proposed system to guide tourists to visit the specified place. The evaluation results of a pilot study shows that the proposed guidance system has significantly improved the tourist experiences.},
keywords={augmented reality;computer games;history;travel industry;tour guiding system;historical relics;Yuanmingyuan;relic park;cultural relics;game-based guidance system;time travel game;MAGIC-EYES;augmented reality technology;interactive modes;tourist experiences;Augmented reality;Games;Buildings;History;Cultural differences;Multimedia communication;Guidance system;Augmented reality;Game-based learning;Location-based},
doi={10.1109/VR.2016.7504776},
ISSN={2375-5334},
month={March},}
@INPROCEEDINGS{7441110,
author={J. A. {Frank} and A. {Brill} and V. {Kapila}},
booktitle={2016 Indian Control Conference (ICC)},
title={Interactive mobile interface with augmented reality for learning digital control concepts},
year={2016},
volume={},
number={},
pages={85-92},
abstract={The use of augmented reality (AR) and mobile applications has recently been investigated in the teaching of advanced concepts and training of skills in a variety of fields. By developing educational mobile applications that incorporate augmented reality, unique interactive learning experiences can be provided to learners on their personal smartphones and tablet computers. This paper presents the development of an immersive user interface on a tablet device that can be used by engineering students to interact with a motor test-bed as they examine the effects of discrete-time pole locations on the closed-loop dynamic response of the test-bed. Specifically, users point the rear-facing camera of the tablet at the test-bed on which colored markers are affixed to enable an image processing routine running on the tablet to measure the angular position of an arm attached to the motor. To perform vision-based control of the angular position of motor arm, a discrete-time Kalman filter and a full-state feedback controller are implemented in the background of the application. As the user taps on the touchscreen of the device, s/he adjusts the angular position of a 3D semi-transparent virtual arm that represents the set point to the system. An interactive pole-zero plot allows users to tap at any desired location for the closed-loop pole-placement, in turn triggering the application code to redesign a new controller for driving the test-bed. Real-time plots enable the user to explore the resulting closed-loop response of the test-bed. Experimental results show several responses of the test-bed to demonstrate the efficacy of the proposed system.},
keywords={augmented reality;closed loop systems;computer aided instruction;computer vision;control engineering computing;control engineering education;discrete time systems;image colour analysis;image sensors;mobile computing;notebook computers;pole assignment;position control;state feedback;user interfaces;closed-loop response;closed-loop pole-placement;interactive pole-zero plot;3D semitransparent virtual arm;full-state feedback controller;vision-based control;arm angular position;colored markers;rear-facing camera;closed-loop dynamic response;discrete-time pole locations;motor test-bed;engineering students;tablet device;immersive user interface;educational mobile applications;AR;digital control concepts learning;augmented reality;interactive mobile interface;Augmented reality;Mobile applications;User interfaces;Cameras;Image processing;Position measurement;Mobile handsets},
doi={10.1109/INDIANCC.2016.7441110},
ISSN={null},
month={Jan},}
@INPROCEEDINGS{8686716,
author={P. {Putiorn} and R. {Nobnop} and P. {Buathong} and K. {Soponronnarit}},
booktitle={2018 Global Wireless Summit (GWS)},
title={Understanding Teachers' Perception Toward the Use of an Augmented Reality-Based Application for Astronomy Learning in Secondary Schools in Northern Thailand},
year={2018},
volume={},
number={},
pages={77-81},
abstract={Augmented Reality (AR) as a technology is evolving along with increasing use of human technology. There is no doubt that a Smartphone is an important device that everyone can carry and use in their everyday life. The integration of the mobile device and AR technology has been increasingly adopted in various fields including Education. In this paper, we present the developmental process of an AR application to facilitate science learning for high school students in Chiang Rai province, Thailand. The AR-based Astronomy application was developed and implemented via students' Smartphones, and used as a medium of interaction in the classroom environment. Furthermore, to assess the usefulness of this application from the teachers' point of view, the experiment was conducted with secondary school students, and then evaluated by 38 pre-service teachers. Overall, the results showed benefits in using the AR application for improving students' learning engagement and enjoyment. Additionally, female pre-service teachers observed that the students perceived more enjoyment when compared with male teachers, who felt that students faced more pressure/tension while using the AR application. The usability test significantly indicated that female teachers preferred to use AR in their class more frequently than their counterparts. Moreover, the disparity between urban and rural schools still remains, where rural teachers found that AR technology would be complicated to use in rural schools' context.},
keywords={astronomy computing;augmented reality;computer aided instruction;educational institutions;smart phones;female pre-service teachers;male teachers;AR application;female teachers;urban schools;rural schools;rural teachers;AR technology;secondary schools;northern Thailand;mobile device;high school students;Chiang Rai province;classroom environment;secondary school students;augmented reality-based application;astronomy learning;smart phone;preservice teachers;Thailand;Mobile Learning;Augmented Reality;Interactive Media;Astronomy},
doi={10.1109/GWS.2018.8686716},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8798058,
author={M. E. {Kouzi} and A. {Mao} and D. {Zambrano}},
booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
title={An Educational Augmented Reality Application for Elementary School Students Focusing on the Human Skeletal System},
year={2019},
volume={},
number={},
pages={1594-1599},
abstract={Augmented Reality (AR) as a new field regarding Human Computing Interaction (HCI) has been gaining momentum in the last few years. Being able to project interactive graphics into real-life environments can be applied in various fields, research and commercial goals. In the field of education, textbooks are still considered to be the primary tool used by students to learn about new topics. Since AR requires interaction and exploration, it brings a ludic component that is hard to replicate using regular textbooks. The application we developed allows elementary school students to interact with a fully three-dimensional human skeleton model, using specialized virtual buttons. Students can understand this complex structure and learn the names of important bones just by using a tablet, a picture and their hands. Results show that the majority of students consider that our AR application helped them visualize and learn more about the human skeletal system. Additionally, the data we gathered shows that there was a 16% increase in correct responses regarding bone names after using our AR application. Our AR application successfully helped the students learn about the human skeletal system by introducing them to AR technologies.},
keywords={augmented reality;biology computing;computer aided instruction;human computer interaction;educational augmented reality application;elementary school students;human skeletal system;interactive graphics;commercial goals;three-dimensional human skeleton model;AR application;human computing interaction;specialized virtual buttons;AR technologies;Three-dimensional displays;Bones;Solid modeling;Games;Augmented reality;Education;Visualization;Augmented Reality;Human Computer Interaction (HCI);Education;Elementary School;Skeleton System;3D object},
doi={10.1109/VR.2019.8798058},
ISSN={2642-5246},
month={March},}
@INPROCEEDINGS{8736156,
author={N. {Thiwanka} and U. {Chamodika} and L. {Priyankara} and S. {Sumathipala} and G. T. {Weerasuriya}},
booktitle={2018 3rd International Conference on Information Technology Research (ICITR)},
title={Augmented Reality Based Breadboard Circuit Building Guide Application},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Building circuits on breadboards is an activity which requires a lot of attention and thinking. If there is a way to guide this process by using modern technologies, the learning process can be made more effective and interactive. This study proposes a solution that provides students with an augmented reality visualization of the expected circuit on a breadboard before they actually make the circuit. The proposed system can be divided into four main modules based on their functionality (a) extracting possible information from the electronic components, (b) scanning circuit diagrams for identifying circuit symbols and their connectivity, (c) finding the appropriate arrangement of the electronic components on the breadboard and (d) using augmented reality to visualize the circuit on a breadboard. This solution provides an innovative approach to facilitate the learning process of students by making electronic circuit building interesting and interactive.},
keywords={analogue circuits;augmented reality;circuit diagrams;data visualisation;electronic engineering computing;learning process;augmented reality visualization;expected circuit;electronic components;scanning circuit diagrams;circuit symbols;augmented reality based breadboard circuit building guide application;Breadboard;Image color analysis;Resistors;Augmented reality;Logic gates;image processing;augmented reality;convolutional neural networks;deep learning;transfer learning},
doi={10.1109/ICITR.2018.8736156},
ISSN={null},
month={Dec},}
@INPROCEEDINGS{8567255,
author={O. {Güler} and I. {Yücedağ}},
booktitle={2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)},
title={Developing an CNC lathe augmented reality application for industrial maintanance training},
year={2018},
volume={},
number={},
pages={1-6},
abstract={In this study, developing an augmented reality training application for CNC (computer numerical control) lathe looms for industrial maintenance and repair training was described. The content has been developed for the CNC LATHE TEARS module, which is trained in the field of Machine Technologies to be operated on mobile like smart phone, tablet, etc. devices. First a training scenario was prepared for the development of the application. Then three dimensional model of a CNC lathe was modelled. Models designed to develop the augmented reality application have been transferred to the Unity3D game engine software, and using the Vuforia plug-in, a marker-based augmented reality application has been developed to work on android operating system based mobile devices like smartphone, tablet, etc. It is considered that using the developed application in the training of the students of the Machine Technologies Area in the institutions providing vocational and technical education in the official and private institutions affiliated to the Ministry of National Education, motivation of the students and the success rates of the courses will increase. At the same time, the developed application will provide facilitate the training of the users, increase the quality in production and faster maintenance and installation.},
keywords={augmented reality;computer based training;computerised numerical control;maintenance engineering;mobile computing;production engineering computing;smart phones;industrial maintanance training;augmented reality training application;computer numerical control;industrial maintenance;repair training;CNC LATHE TEARS module;Unity3D game engine software;marker-based augmented reality application;mobile devices;Android operating system;machine technologies area;Training;Maintenance engineering;Software;Augmented reality;Cameras;Computer numerical control;augmented reality;CNC Lathe;mobile software;interactive system},
doi={10.1109/ISMSIT.2018.8567255},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{7961559,
author={V. {Ferrer} and A. {Perdomo} and H. R. {Ali} and C. {Fies} and J. {Quarles}},
booktitle={2017 IEEE Virtual Reality Workshop on K-12 Embodied Learning through Virtual Augmented Reality (KELVAR)},
title={Virtual humans for temperature visualization in a tangible augmented reality educational game},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Our primary objective is to enable effective game based learning approaches in tangible augmented reality. In game based learning there is often a tradeoff in motivation between the educational aspects and game aspects. For example, consider our previous work - a tangible augmented reality application for passive solar energy education (AR-SEE), in which users learn about the science behind architectural design by interacting with a tangible model house and an augmented reality-based visualization of energy transfer within the house. This research extends AR-SEE to begin to convert this educational simulation into an effective educational game by introducing gaming elements, such as interactive virtual humans. Although it is known that AR-SEE does enable learning, it is unknown how the addition of interactive virtual humans will affect user perception of temperature data and learning. In this paper, the goal was to compare user perception of two approaches to temperature data visualization in in tangible augmented reality on mobile phones: (1) the current particle-based visualization (i.e., based on the science of energy transfer) and (2) novel virtual human-based visualizations. The game was intended for high school students. However, as a preliminary study, we conducted a user study with 27 3rd and 4th year architecture students that compared these two visualization approaches and their impact on temperature estimation, motivation, and perceived learning effectiveness. In the future, we plan to integrate this game into high school curricula.},
keywords={augmented reality;computer aided instruction;computer games;data visualisation;educational courses;educational institutions;human factors;mobile computing;virtual humans;temperature visualization;tangible augmented reality educational game;educational aspects;game aspects;educational simulation;mobile phones;particle-based visualization;virtual human-based visualizations;high school curricula;Estimation;Education;Cameras;Measurement;Solar energy;Heating systems;Augmented reality;education;visualization},
doi={10.1109/KELVAR.2017.7961559},
ISSN={null},
month={March},}
@INPROCEEDINGS{8576887,
author={M. {Xi} and M. {Adcock} and J. {McCulloch}},
booktitle={2018 IEEE Workshop on Augmented and Virtual Realities for Good (VAR4Good)},
title={Future Agriculture Farm Management using Augmented Reality},
year={2018},
volume={},
number={},
pages={1-3},
abstract={Augmented reality (AR) technology is blooming in the past few years with a growing number of low-cost AR devices becoming available to the general public. AR techniques have demonstrated the capacity to optimise task efficiency in a broad range of industries and provide engaging entertainment and education experiences. However, the potential of AR has not yet been fully explored. One of the extremely underexplored areas is its application in broad agriculture sector. As a major source of food, agriculture has always been a national priority. Agriculture farming is highly labour-intensive and heavily relies on individual farmer's expertise, resulting in challenging farm management issues. We argue that AR can make critical contributions to the optimum management of agriculture farms. We take aquaculture ponds as an example, and presented three use cases to show how AR can potentially support more efficient farm management activities: water quality management, remote collaboration, and boardroom discussion.},
keywords={agricultural engineering;agriculture;aquaculture;augmented reality;decision making;food products;water quality;water quality management;augmented reality technology;decision making;agriculture farm management;food source;aquaculture ponds;remote collaboration;boardroom discussion;Aquaculture;Augmented reality;Collaboration;Task analysis;Training;Biomass;Augmented reality;interactive data visualisation;decision making;farm management;H.5.1 [Information Interfaces and Presentation (e.g., HCI)]: Multimedia Information Systems;Artificial, augmented, and virtual realities},
doi={10.1109/VAR4GOOD.2018.8576887},
ISSN={null},
month={March},}
@INPROCEEDINGS{7359574,
author={F. {Sorrentino} and L. D. {Spano} and R. {Scateni}},
booktitle={2015 International Conference on Interactive Mobile Communication Technologies and Learning (IMCL)},
title={Speaky Notes Learn languages with augmented reality},
year={2015},
volume={},
number={},
pages={146-150},
abstract={In recent years, mobile devices have become very popular within young people. Thanks to developments in mobile technologies, these devices can now do much more than just voice calls and texts. We envision mobile devices as tools for improving the young users' lifestyle, especially for learning. In this work we present a web authoring system that makes it possible to create a mobile application that supports children in learning a new language in a more pleasant and entertaining way by using Augmented Reality. This application allows pupils to improve their speaking skills turning the language acquisition into a game under the supervision of both teachers and parents. Our contribution is focused on understanding how digital technology can facilitate learning while keeping in mind that it is a wide and interdisciplinary issue.},
keywords={augmented reality;mobile computing;mobile handsets;augmented reality;mobile devices;mobile technologies;Web authoring system;speaking skills;language acquisition;Augmented reality;Education;Games;Smart phones;Mobile communication;Vocabulary;Learning;Child;Augmented Reality;Voice;Language},
doi={10.1109/IMCTL.2015.7359574},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8951995,
author={M. Z. {Iqbal} and E. {Mangina} and A. G. {Campbell}},
booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
title={Exploring the Use of Augmented Reality in a Kinesthetic Learning Application Integrated with an Intelligent Virtual Embodied Agent},
year={2019},
volume={},
number={},
pages={12-16},
abstract={Technology in education is rapidly changing the way that students learn. This allows for the creation of learning tools that provide better interaction, creative engagement and adaptability to a learner. Augmented Reality (AR) is one of these emerging technologies which can facilitate the development of new learning tools. AR has successfully been proven to allow new types of learning pedagogies by providing human-centered learning environments. In particular, the pedagogical approach of Kinesthetic learning or ”Learning by Doing” has not been explored in great detail in combination with Augmented Reality. This approach is to physically act out an activity to aid in the learning process and has been previously proven as one of the most successful approaches. For a successful application of this pedagogy, the student must get precise feedback and be guided through a process, thus some form of intelligent guide needs to be actively monitoring the learning environment. This paper presents the exploration of this concept through the presentation of an initial prototype system that was developed and implemented based on an adaptive learning methodology within an AR application, with the prospect that in the future will use intelligent agents.},
keywords={Human-centered-computing;Interaction-paradigms—Mixed-/-Augmented-reality;Applied-computing—E-learning;Applied-computing—Interactive-learning-environments},
doi={10.1109/ISMAR-Adjunct.2019.00018},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8584190,
author={A. {Medina-Carrión} and P. {Arias-Espinoza} and V. {Robles-Bykbaev} and Y. {Robles-Bykbaev} and F. {Pesántez-Avilés} and J. {Ortega}},
booktitle={2018 Congreso Argentino de Ciencias de la Informática y Desarrollos de Investigación (CACIDI)},
title={An interactive educational tool based on augmented reality, mobile applications and comic strips to teach children the Cañari and Inca cultures in the Ecuadorian context},
year={2018},
volume={},
number={},
pages={1-5},
abstract={According to the United Nations Educational, Scientific and Cultural Organization (UNESCO), due of the rapidity of cultural change, safeguarding the world's tangible and intangible cultural heritage has become an increasingly complex and multidimensional undertaking. Therefore, it is fundamental for the children and youth of today to learn the culture, values, and traditions that define their identity as well as the identity of their nation. For these reasons in this paper, we present an interactive app aimed at supporting both teaching and rescuing the heritage of Cañari and Inca indigenous cultures. Our mobile app can be used at children's home or during the guided visits of one of the most important museums in Ecuador: the Pumapungo Museum. Likewise, the app can identify QR codes to show multimedia material to children, contains several 3D objects that are presented using Augmented Reality (AR), and incorporates a Natural Language Processing (NLP) module to determine the children's learning progress. In this line, it is important mentioning that children use the app to create small "stories" about the learned concepts. The app was tested in real guided visits in the museum with 30 children from three different schools (low and middle -income), and the results show high levels of interest in the contents and the app.},
keywords={augmented reality;computer aided instruction;educational institutions;history;mobile computing;museums;natural language processing;teaching;mobile app;augmented reality;mobile applications;interactive app;teach children;cultural heritage;Pumapungo museum;QR codes;natural language processing;NLP;UNESCO;United Nations Educational-Scientific and Cultural Organization;Cultural differences;Augmented reality;Three-dimensional displays;Natural language processing;Tools;Mobile applications;Multimedia systems;Natural language processing (NLP);Augmented reality (AR);Interactive Application;Expert System;Mobile applications},
doi={10.1109/CACIDI.2018.8584190},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8797897,
author={A. {Klippel} and J. O. {Wallgrün} and A. {Masrur} and J. {Zhao} and P. {LaFemina}},
booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
title={Warping Space and Time - xR Reviving Educational Tools of the 19th Century},
year={2019},
volume={},
number={},
pages={1022-1023},
abstract={xR has the potential to warp both space and time. We demonstrate this potential by designing a mixed reality application for mobile devices for the Penn State's Obelisk, a historic landmark on the main Penn State campus that artistically reveals the geological history of Pennsylvania. Our AR application allows for placing a model of the Obelisk on any surface, interacting with the individual stones to reveal their geological characteristics and location of excavation, and changing to an immersive VR experience of this location based on 360° imagery. Originally conceptualized as a teaching tool for the School of Mines, our xR application revives the Obelisk's long forgotten mission and allows educators to integrate it once more into the curriculum as well as creatively expand its potential.},
keywords={augmented reality;computer aided instruction;educational institutions;geology;history;mobile computing;teaching;xR reviving educational tools;mixed reality application;mobile devices;historic landmark;AR application;geological characteristics;immersive VR experience;teaching tool;xR application;Penn State campus;Penn State obelisk;Pennsylvania;geological history;Solid modeling;Geology;Three-dimensional displays;Augmented reality;Media;Tools;Augmented reality;mixed reality;interactive learning;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;K.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology},
doi={10.1109/VR.2019.8797897},
ISSN={2642-5246},
month={March},}
@INPROCEEDINGS{7815575,
author={N. L. {Lescano} and S. E. {Mamani} and J. G. {Illatopa}},
booktitle={2016 IEEE XXIII International Congress on Electronics, Electrical Engineering and Computing (INTERCON)},
title={Cultiventura software architecture tool supporting the learning of the moche culture},
year={2016},
volume={},
number={},
pages={1-6},
abstract={This paper aims to describe the software architecture used for the development of Cultiventura, tool that provides technology resources to support the formation of cultural identity in the teaching-learning process through technology of videogame and augmented reality for students of fourth, fifth and sixth elementary school. The design method of architecture in the context of an agile software development process is described. The results indicate that applying the model based on architecture has allowed us to develop the application with better management from the conceptual vision to the software artifact, understanding more appropriately as the essential features are implemented in the organization of the elements Cultiventura taking advantage of the interactive features of the technology to insert cultural concepts that allow students to bring cultural identification through fun and learning.},
keywords={augmented reality;computer aided instruction;computer games;history;software architecture;software tools;teaching;Cultiventura software architecture tool;Moche culture learning;video game;augmented reality;teaching-learning process;software artifact;cultural identification;Games;Cultural differences;Computer architecture;Augmented reality;Software;Software architecture;Engines;Architecture;games;augmented reality;movile;software},
doi={10.1109/INTERCON.2016.7815575},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{7300745,
author={P. V. d. F. {Paiva} and L. d. S. {Machado} and T. V. V. {Batista}},
booktitle={2015 XVII Symposium on Virtual and Augmented Reality},
title={A Collaborative and Immersive VR Simulator for Education and Assessment of Surgical Teams},
year={2015},
volume={},
number={},
pages={176-185},
abstract={Traditionally, the evaluation of surgical procedures in VR simulators have been restricted to their individual technical aspects, and disregarded the procedures carried out by teams. However, some decision models have been proposed in order to be incorporated and support the collaborative training evaluation process of surgical teams in Collaborative Virtual Environments (CVEs). This paper aims to discuss some possibilities and advantages of using the CVEs for the training process and assessment of surgical teams as well as presenting the steps of planning and development of a new simulator with these features, called SimCEC. The discussion is held in a computational modelling and implementation optic.},
keywords={biomedical education;computer based training;groupware;medical computing;surgery;virtual reality;collaborative virtual environments;immersive VR simulator;collaborative training evaluation process;surgical team assessment;SimCEC;CVE;education;Collaboration;Surgery;Training;Computational modeling;Servers;Real-time systems;surgical simulation;collaborative virtual environments;user's assessment;SimCEC;VR},
doi={10.1109/SVR.2015.33},
ISSN={null},
month={May},}
@INPROCEEDINGS{8951926,
author={F. {Bork} and A. {Lehner} and D. {Kugelmann} and U. {Eck} and J. {Waschke} and N. {Navab}},
booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
title={VesARlius: An Augmented Reality System for Large-Group Co-located Anatomy Learning},
year={2019},
volume={},
number={},
pages={122-123},
abstract={Interactive educational environments are one of the prime applications for the use of Augmented Reality (AR). A large variety of such systems has been proposed in the past for various areas of education. However, in most cases the number of users these AR systems can support is limited. Only few systems have been developed that support a large number of co-located users to jointly collaborate in a dynamic and interactive learning environment. Multi-user AR collaboration presents a unique setting with distinct challenges and requirements for user interaction and information sharing. In this paper, we present VesARlius, a novel AR system for collaborative and interactive anatomy learning in a large group of co-located users. Our system employs a set of multi-user collaboration paradigms allowing users to engage in an interactive AR learning environment. We evaluated the collaborative features of our system in a user study with 16 medical students. Results demonstrate the potential of the VesARlius system to be used effectively for large-group AR anatomy learning. From our lessons learned, we provide a set of design guidelines for developing similar AR systems to enable large-group collaboration in other application domains.},
keywords={},
doi={10.1109/ISMAR-Adjunct.2019.00-66},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8760886,
author={A. P. {Silva} and F. R. {Ribeiro} and R. {Matos} and A. {Silva} and I. {Vieira} and P. {Beato}},
booktitle={2019 14th Iberian Conference on Information Systems and Technologies (CISTI)},
title={Trilho Verde: Enhancing field trips with mobile learning},
year={2019},
volume={},
number={},
pages={1-6},
abstract={In this article we present a case study developed in the area of Mobile Learning where an interactive mobile application is used to support learning in the context of a field trip. This tool will guide students along a pre-established itinerary, notifying them of relevant locations found along the way. In each of these key places, a set of activities are presented in order to complement the information that can be inferred from the observation of some specific natural elements and/or to test the students' knowledge. With this work, we hope to contribute to the enrichment of the range of teaching/learning tools available to schools, especially those where resources are scarcer. The adopted developing methodology was the design science approach. The feedback received from the school during the Demonstration phase, where a first prototype was presented, was very positive, giving us the necessary motivation to improve the designed application and complete the last two phases of this approach in future work.},
keywords={computer aided instruction;educational institutions;interactive systems;mobile learning;mobile learning;interactive mobile application;learning tool;teaching tool;learning support;Trilho Verde;field trips;Tools;Global Positioning System;Geology;Mobile applications;Prototypes;Augmented reality;Field Trips;Game-based learning;Mobile Learning;Situated Learning},
doi={10.23919/CISTI.2019.8760886},
ISSN={2166-0727},
month={June},}
@INPROCEEDINGS{7753401,
author={A. {Protopsaltis} and M. {Mentzelopoulos} and J. {Ferguson} and K. {Kaloyan}},
booktitle={2016 11th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)},
title={Quiz Cube: An AR mobile learning application},
year={2016},
volume={},
number={},
pages={151-155},
abstract={The current paper presents the Quiz Cube application and its evaluation. The Quiz Cube application is an AR mobile learning application for students and teachers to easily make and use AR UI system using fiducial marker cubes. AR as a platform is just now reaching its full potential. Since smartphones and mobile devices are now at a sufficiently large user base, it is worth looking at the potential for an extremely small form factor delivery system that is flexible, easily modified, and used by educators and students. An easily modifiable AR learning experience will present an AR Mobile platform development, interactive museums, and the chosen subject in a new style. This method can be shown to improve not only knowledge of the chosen subject through investigation, but a better understanding of development potentials for the mobile devices now ubiquitous to students. The Quiz Cube application was evaluated in three different ways and the results are presented here.},
keywords={augmented reality;mobile learning;smart phones;user interfaces;quiz cube;AR mobile learning;AR UI system;fiducial marker cubes;smartphones;factor delivery system;Mobile communication;Cameras;Smart phones;Augmented reality;Software;Three-dimensional displays;mobile learning;Augmented Reality (AR);User Interface (UI);Quick Response Code (QR)},
doi={10.1109/SMAP.2016.7753401},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{7991093,
author={K. {Lee} and Y. {Chen} and H. {Hsieh} and K. {Chin}},
booktitle={2017 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)},
title={Application of intuitive mixed reality interactive system to museum guide activity},
year={2017},
volume={},
number={},
pages={257-258},
abstract={The old museum guide modes use phonetic system or traditional artificial guide, failing to combine the virtual information into the real world, and the artificial guide sometimes needs a great deal of manpower. Therefore, this study aims to develop a mixed reality interactive system with gesture recognition technique, and combines smart phone with Cardboard to generate the mixed reality guidance service, so as to provide an intuitive museum guide mode. This new technological application mode is expected to enable people to obtain the required information from the real environment in easier ways, so that the users can perform intuitive interaction with the 3D object and feel their personally on the scene. This system is expected to be integrated and introduced into different application contexts in the future, to be popularized to different research areas and related industries.},
keywords={augmented reality;museums;smart phones;intuitive mixed reality interactive system;museum guide activity;virtual information;gesture recognition technique;smart phone;Cardboard;mixed reality guidance service;intuitive museum guide mode;3D object;Virtual reality;Three-dimensional displays;Smart phones;Gesture recognition;Education;Google;Mixed reality;Cardboard;museum guide;smart phone},
doi={10.1109/ICCE-China.2017.7991093},
ISSN={null},
month={June},}
@INPROCEEDINGS{8500545,
author={Y. {Feng} and C. {Yu} and S. {Xu} and H. X. {Liu} and H. {Peng}},
booktitle={2018 IEEE Intelligent Vehicles Symposium (IV)},
title={An Augmented Reality Environment for Connected and Automated Vehicle Testing and Evaluation*},
year={2018},
volume={},
number={},
pages={1549-1554},
abstract={Testing and evaluation are critical steps in the development of connected and automated vehicle (CAV) technology. One limitation of closed CAV testing facilities is that they merely provide empty roadways, in which testing CAVs can only interact with a limited number of other CAVs and infrastructure. This paper presents an augmented reality environment for CAV testing and evaluation. A real-world testing facility and a simulation platform are combined together. Movements of testing CAVs in the real world are synchronized with simulation and information of background traffic is fed back to testing CAVs. Testing CAVs can interact with virtual background traffic as if in a realistic traffic environment. The proposed system mainly consists of three components: a simulation platform, testing CAVs, and a communication network. Testing scenarios that have safety concerns and/or require interactions with other vehicles can be performed. Two exemplary test scenarios are designed and implemented to demonstrate the capabilities of the system.},
keywords={augmented reality;interactive systems;traffic engineering computing;augmented reality environment;automated vehicle testing;closed CAV testing facilities;exemplary test scenarios;virtual background traffic interaction;Testing;Augmented reality;Delays;Load modeling;Roads;Synchronization;Communication networks},
doi={10.1109/IVS.2018.8500545},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{7300736,
author={P. R. J. d. {Reis} and C. E. F. {Matos} and P. S. {Diniz} and D. M. {Silva} and W. {Dantas} and G. {Braz} and A. C. d. {Paiva} and A. S. {Araújo}},
booktitle={2015 XVII Symposium on Virtual and Augmented Reality},
title={An Immersive Virtual Reality Application for Collaborative Training of Power Systems Operators},
year={2015},
volume={},
number={},
pages={121-126},
abstract={The use of immersive Virtual Reality applications for training in industrial areas has been increasing due to the benefits related to that technology. This paper presents an application to perform training of power system operators in a collaborative and immersive environment. This application aims to enhance the user immersion and increase collaborative training in a Virtual Reality using Collaborative Virtual Environment and a Problem Based Learning approach. It was build in Unity engine and presents a fully integrated scenario of power system visualization with a supervisor module that improves training through the simulation of real events.},
keywords={computer based training;data visualisation;power engineering computing;virtual reality;immersive virtual reality application;collaborative training;power systems operators;industrial areas;power system operator training;immersive environment;user immersion;collaborative virtual environment;problem based learning approach;Unity engine;power system visualization;supervisor module;Visualization;Three-dimensional displays;Mice;Collaboration;Training;Virtual reality;Power systems;Virtual Reality;Collaborative Virtual Environment;Power System},
doi={10.1109/SVR.2015.24},
ISSN={null},
month={May},}
@INPROCEEDINGS{8516483,
author={J. {Luna} and R. {Treacy} and T. {Hasegawa} and A. {Campbell} and E. {Mangina}},
booktitle={2018 IEEE Games, Entertainment, Media Conference (GEM)},
title={Words Worth Learning - Augmented Literacy Content for ADHD Students},
year={2018},
volume={},
number={},
pages={1-9},
abstract={3-9% school-aged children in Ireland are estimated to be affected by ADHD (Attention Deficit Hyperactivity Disorder), according to HSE (Health Service Executive). Typical co-morbid conditions include: anxiety disorder, oppositional defiant disorder, conduct disorder, depression, sleep problems, epilepsy, learning difficulties, etc. As such, unless early intervention properly takes place, performance of children with ADHD at school tends to be compromised (e.g. leaving school early and substance abuse etc.). The current work presents a preliminary investigation of creating 3D Learning Objects (3DLO) using Augmented Reality (AR), following the IEEE Learning Objects standards, to enhance an established online literacy programme, WordsWorthLearning (WWL). The methodology and experimentation of creating AR 3DLO is proposed, followed by a pilot evaluation, aiming to provide a foundation of a system that can support interactive educational content, service, assessment, and feedback for children with ADHD and their parents and teachers.},
keywords={augmented reality;computer aided instruction;medical disorders;interactive educational content;words worth learning;ADHD students;Ireland;HSE;typical co-morbid conditions;oppositional defiant disorder;AR 3DLO;school-aged children;attention deficit hyperactivity disorder;augmented reality;online literacy programme;health service executive;3D learning objects;WWL;Three-dimensional displays;Solid modeling;Shape;Animation;Cameras;Games;Two dimensional displays;Augmented Reality (AR);3D Learning Objects;Attention Deficit Hyperactivity Disorder (ADHD);Assistive Technology},
doi={10.1109/GEM.2018.8516483},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{7495421,
author={C. {Arcos} and W. {Fuertes} and C. {Villacís} and M. {Zambrano} and T. {Noboa} and A. {Tacuri} and H. {Aules} and T. {Toulkeridis}},
booktitle={2016 18th Mediterranean Electrotechnical Conference (MELECON)},
title={Playful and interactive environment-based augmented reality to stimulate learning of children},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Our study is about how to create educational software using Augmented Reality combined with Bloom's Taxonomy, whose main purpose is to stimulate spatial reasoning improving the learning process of children. To reach this main objective, we have applied an incremental Object-Oriented Hypermedia Design Method, with the purpose to produce a mobile application for a learning basis on Smart-Client architecture. Furthermore, we incorporated operations necessary for cognitive development and effective learning in the logic of the application. Therefore, we applied modern 3D technology approaches such as Unity 3D Game Engine and Vuforia. Additionally, we designed and implemented a Computerized Classification Test based on the Bloom's taxonomy in order to determine the learning results. The validation of our proposed solution has been engaged by testing them in two representative public schools. The results demonstrate that this educational software for learning stimulates the cognitive development and improves the comprehension of children, with an effective quantitative assessment based on a constructivist paradigm.},
keywords={Software;Three-dimensional displays;Education;Augmented reality;Planets;Taxonomy;Games;Augmented Reality;educational software;Bloom Taxonomy;Vuforia},
doi={10.1109/MELCON.2016.7495421},
ISSN={2158-8481},
month={April},}
@ARTICLE{8891943,
author={M. {Vazquez Briseno} and O. N. {Gomez Soto} and A. {Marquez Tellez} and J. I. {Nieto Hipolito} and S. {Infante Prieto} and J. d. D. {Sanchez Lopez}},
journal={IEEE Latin America Transactions},
title={Enhancing Nutrition Learning Using Interactive Tools},
year={2019},
volume={17},
number={05},
pages={751-758},
abstract={Nowadays, obesity is a serious health problem worldwide. Its occurrence has exponentially increased in both children and adults, especially in developing countries. Since overweight and obese children are more likely to be overweight or obese adults, it is important to control and prevent childhood obesity. Moreover, overweight and obesity can put children at higher risk for various health problems. Recently, several information and communication technology tools have been designed to promote good health habits, physical activity, nutrition education, and weight loss. Most of these tools, however, target at an adult audience and fail to capture the attention of children. To address this gap, we present two child-friendly prototypes as nutrition education tools for preventing obesity. The prototypes rely on computational techniques such as augmented reality and serious games to provide children a more user-friendly experience. We also present the designs and evaluation of the prototypes and provide key guidelines for developers.},
keywords={augmented reality;computer games;health care;medical computing;mobile computing;paediatrics;adult audience;child-friendly prototypes;nutrition education tools;preventing obesity;serious games;nutrition learning;interactive tools;serious health problem;overweight children;obese children;childhood obesity;communication technology tools;health habits;Pediatrics;Tools;Obesity;Prototypes;Software;IEEE transactions;Education;Augmented reality;Sensors;Serious games;Mobile Health;Mobile software},
doi={10.1109/TLA.2019.8891943},
ISSN={1548-0992},
month={May},}
@INPROCEEDINGS{8951891,
author={R. {Cao} and Y. {Liu}},
booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
title={Hand ControlAR: An Augmented Reality Application for Learning 3D Geometry},
year={2019},
volume={},
number={},
pages={144-149},
abstract={The traditional way of learning geometry cannot provide a great support for novice students since the geometric figures are 2D on the blackboard or the book. In consideration that Augmented Reality(AR) provides an intuitive way to learn geometry, an interactive AR system that enables students to naturally and directly manipulating 3D objects through hand gesture-based interactions and intuitively explore the spatial relationship between spheres and polyhedrons is proposed in this paper. The proposed gesture-based interaction enables the user manipulate AR objects in the real 3D space instead of 2D space. We design three levels of study to enable students to learn the geometric concepts as well as an experiment to evaluate the effectiveness of the AR system. Analysis of experimental results showed that the proposed system is easy to use, attractive, and helpful for students.},
keywords={Augmented Reality;Hand Gesture Interaction;User Defined Targets;3D Objects Manipulation;Geometry Education},
doi={10.1109/ISMAR-Adjunct.2019.00-60},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8802492,
author={A. S. {Rodrigues Ancioto} and L. F. {dos Santos Freitas} and M. {de Paiva Guimarães}},
booktitle={2018 20th Symposium on Virtual and Augmented Reality (SVR)},
title={Simulator for Teaching Magnetic Disk Scheduling Algorithms},
year={2018},
volume={},
number={},
pages={65-74},
abstract={Disk scheduling algorithms are fundamental to operational systems, managing schedule I/O requests arriving for a disk. They aim to make the systems efficient, fast, and fair, allowing to improve the seek time. This papeŕs goal is to present an immersive and interactive virtual reality simulator for teaching magnetic disk scheduling algorithms. This application simulates the First Come - First Served (FCFS) algorithm. The results of a usability and learning are presented.},
keywords={computer aided instruction;computer science education;computer simulation;magnetic disc storage;scheduling;teaching;virtual reality;magnetic disk scheduling algorithms;operational systems;interactive virtual reality simulator;immersive virtual reality simulator;teaching;I/O requests;first come-first served algorithm;FCFS;Two dimensional displays;Scheduling algorithms;Java;Three-dimensional displays;Education;Augmented reality;disk scheduling;memory management;operational system;virtual reality},
doi={10.1109/SVR.2018.00021},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{7988648,
author={Y. {Lee} and C. {Shin} and A. {Plopski} and Y. {Itoh} and T. {Piumsomboon} and A. {Dey} and G. {Lee} and S. {Kim} and M. {Billinghurst}},
booktitle={2017 International Symposium on Ubiquitous Virtual Reality (ISUVR)},
title={Estimating Gaze Depth Using Multi-Layer Perceptron},
year={2017},
volume={},
number={},
pages={26-29},
abstract={In this paper we describe a new method for determining gaze depth in a head mounted eye-tracker. Eye-trackers are being incorporated into head mounted displays (HMDs), and eye-gaze is being used for interaction in Virtual and Augmented Reality. For some interaction methods, it is important to accurately measure the x-and y-direction of the eye-gaze and especially the focal depth information. Generally, eye tracking technology has a high accuracy in x-and y-directions, but not in depth. We used a binocular gaze tracker with two eye cameras, and the gaze vector was input to an MLP neural network for training and estimation. For the performance evaluation, data was obtained from 13 people gazing at fixed points at distances from 1m to 5m. The gaze classification into fixed distances produced an average classification error of nearly 10%, and an average error distance of 0.42m. This is sufficient for some Augmented Reality applications, but more research is needed to provide an estimate of a user's gaze moving in continuous space.},
keywords={augmented reality;cameras;gaze tracking;helmet mounted displays;image classification;learning (artificial intelligence);multilayer perceptrons;gaze depth;head mounted eye-tracker;head mounted displays;virtual reality;augmented reality;focal depth information;eye tracking technology;binocular gaze tracker;eye cameras;gaze vector;MLP neural network;training;performance evaluation;gaze classification;fixed distances;fixed points;average classification error;gaze depth estimation;multilayer perceptron;Three-dimensional displays;Meters;Cameras;Training;Error analysis;Resists;Calibration;Eye-gaze;3D gaze;Machine Learning;Augmented Reality;Head-mounted display},
doi={10.1109/ISUVR.2017.13},
ISSN={null},
month={June},}
@ARTICLE{8812921,
author={L. {Cen} and D. {Ruta} and L. {Mahmoud Mohd Said Al Qassem} and J. {Ng}},
journal={IEEE Transactions on Learning Technologies},
title={Augmented Immersive Reality (AIR) for Improved Learning Performance: A Quantitative Evaluation},
year={2019},
volume={},
number={},
pages={1-1},
abstract={Technology-enhanced learning has attracted increasing attention of educational community focused on improvement of traditional classroom learning. Augmented immersive reality (AIR) technologies enhance users' perception of reality by augmenting it with computer-generated components such as audio, video, 2/3-D graphics, GPS data, etc. The AIR introduces new dimensions of learning experience that ensure better attention, focus and entertainment, thereby boosting students' motivation and attainment. This work presents an award winning AIR-based educational mobile system, code-named AIR-EDUTECH, that was developed to help high school students learn chemistry. The AIR-EDUTECH introduced new AIR features to help students better understand and learn basic concepts of molecular chemistry. It offers immersive 3D visualization and visual interaction with the examined structures that provides a broader and more retentive knowledge and improves intuition around forming basic chemical reactions. The system was introduced and tested in a field study with 45 students in the 11th grade chemistry class, and its impact was evaluated by the formal assessment quiz along with the feedback from survey conducted after the trial. Collected data have been subjected to an in-depth multi-modal quantitative analysis that revealed that AIR-EDUTECH stimulated significant improvements in understanding and retention of the taught content as well as turned learning chemistry into a fun, interesting and interactive experience. It also uncovered a hidden structure of taught knowledge dependencies and highlighted the role that AIR technology could play in reinforcing the retention of critical knowledge that may otherwise widen student knowledge gaps.},
keywords={Education;Chemicals;Three-dimensional displays;Cameras;Visualization;Statistical analysis;Mobile learning;Augmented Reality (AR);Augmented Immersive Reality (AIR);AIR-EDUTECH;education data mining},
doi={10.1109/TLT.2019.2937525},
ISSN={2372-0050},
month={},}
@INPROCEEDINGS{7359593,
author={M. T. {Restivo} and D. {Urbano} and F. {Chouzal}},
booktitle={2015 International Conference on Interactive Mobile Communication Technologies and Learning (IMCL)},
title={Hi kids: That's funny! Mechanics 3D Virtual lab},
year={2015},
volume={},
number={},
pages={232-235},
abstract={The Universidade Júnior, Junior University, is a pioneering initiative of University of Porto taking place during July, since 2005. At the Faculty of Engineering of University of Porto different activities are regularly offered in many of its nine departments and oriented for STEM areas also concerned with trying to contribute to foster young students into these areas. The use of emerging technologies is highly accepted by youngsters. So, some of the activities proposed in the context of the Junior University rely on them. Given the great attractiveness created by the use of remote experimentation, virtual reality and augmented reality with haptics interaction, a new application was developed. It is based on the integration of the Oculus Rift Development Kit 2 with a developed set of virtual reality applications. The present work reports results of the use of the Oculus Rift to create the user emersion in a virtual room where different experiments are available and can be “touched” by using 1 DOF haptic. The authors are concerned in evaluating aspects as motivation, adequateness and its immersive capabilities. This work presents a preliminary evaluation study.},
keywords={augmented reality;computer aided instruction;haptic interfaces;STEM;DOF haptic;Oculus Rift;Oculus Rift Development Kit 2;haptics interaction;augmented reality;virtual reality;STEM;University of Porto;Junior University;mechanics 3D virtual lab;Force;Haptic interfaces;Conferences;Three-dimensional displays;Europe;Springs;Context;Virtual reality;HMD;haptic device;Next-Generation VR technology},
doi={10.1109/IMCTL.2015.7359593},
ISSN={null},
month={Nov},}
@INPROCEEDINGS{8613759,
author={S. {Werrlich} and A. {Daniel} and A. {Ginger} and P. {Nguyen} and G. {Notni}},
booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
title={Comparing HMD-Based and Paper-Based Training},
year={2018},
volume={},
number={},
pages={134-142},
abstract={Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ.},
keywords={augmented reality;computer based training;groupware;helmet mounted displays;human computer interaction;human factors;user interfaces;virtual reality;paper-based training;Collaborative Systems;smartwatches;head-mounted displays;industrial domain;scientific laboratory studies;augmented reality;training tasks;inadequate task complexity;training transfer;engine assembly training task;user satisfaction;system usability scale;user experience questionnaire;Nasa Task Load Index;paper-based instructions;assistive systems;multimodal HMD-based learning;Task analysis;Training;Engines;Software;Resists;Atmospheric measurements;Particle measurements;Augmented Reality;Evaluation;Head Mounted Displays;Training},
doi={10.1109/ISMAR.2018.00046},
ISSN={1554-7868},
month={Oct},}
@INPROCEEDINGS{7975325,
author={Z. {Shareef} and S. {Reddy}},
booktitle={2016 1st India International Conference on Information Processing (IICIP)},
title={PrEduSense: Smart Education Kit for pre-primary classes using Intel® real sense},
year={2016},
volume={},
number={},
pages={1-4},
abstract={The education system has evolved from mere chalk and slate teaching methodology to presentations and smart board used in classrooms nowadays. In order to make teaching more interactive and interesting, applications involving slice of augmented reality are also developed. In this paper PrEduSense: Smart Education Kit for Pre-Primary Classes using Intel® Real Sense is introduced which consists of all the educational modules needed by the school teacher to teach pre-primary students. The concept of Human Computer Interaction is also introduced in this application by incorporating the touchless controller module, gesture recognition module and voice synthesis features of Intel Real Sense Camera. These features make the application further interactive and attractive.},
keywords={cameras;computer aided instruction;gesture recognition;human computer interaction;speech synthesis;teaching;PrEduSense;smart education kit;preprimary classes;teaching;human computer interaction;touchless controller module;gesture recognition module;voice synthesis features;Intel Real Sense Camera;HCI;Cameras;Education;Animals;Three-dimensional displays;Human computer interaction;Augmented reality;Smart phones;Intel® Real Sense Camera Module F200;Gesture Recognition;Voice Synthesis},
doi={10.1109/IICIP.2016.7975325},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{8643204,
author={A. H. {Abdulrazzaq} and M. {Al-Ani}},
booktitle={​Smart Cities Symposium 2018},
title={The needed merge of augmented reality smartphone application with CAS and SDI library services},
year={2018},
volume={},
number={},
pages={1-4},
abstract={In this study two library services, current awareness service (CAS) and selective dissemination of information service (SDI), provided by University of Bahrain, were considered as examples to develop system prototype. The developed system uses Smartphone Augmented Reality (AR) technology to deliver up-to-date information about library services in an interactive way using multimedia to increase researchers' (faculty members and students) awareness. The potentials and other perspectives of the proposed system were discussed. Although it can be further investigated, the proposed system could be considered as a new model for current awareness services of any digital library. Findings revealed that Smartphone AR system provides information in a real environment, giving researchers instant assistance and awareness of their needs, and is a good personal learning tool. However, to make the delivery of Smartphone AR system more effective, suggestions for further investigations are provided like Measuring and reviewing acceptance to the application in terms of Awareness, timeliness, coverage ratio, and usage then collected and summarized by focus groups' thoughts and reactions.},
keywords={Library services;Augmented Reality;Smartphone;Current Awareness Service (CAS);Selective Dissemination of Information (SDI);QR code},
doi={10.1049/cp.2018.1398},
ISSN={null},
month={April},}
@INPROCEEDINGS{7961557,
author={ {Jiayan Zhao} and P. {LaFemina} and J. O. {Wallgrün} and D. {Oprean} and A. {Klippel}},
booktitle={2017 IEEE Virtual Reality Workshop on K-12 Embodied Learning through Virtual Augmented Reality (KELVAR)},
title={iVR for the geosciences},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Field trips are an essential part in many disciplines taught in K12 STEM (Science, Technology, Engineering, and Math) education inside and outside the US such as geography, geosciences, and architecture. Field trips foster embodied experiences of places where students can be situated into an informal learning environment. However, field trips are underutilized due to numerous constraints, a situation that current mass development in immersive technologies promises to eliminate. This paper presents an educational project that aims at creating and empirically evaluating virtual reality (VR) experiences for the geosciences: an interactive volcano experience based on LiDAR (Light Detection And Ranging) and image data of Iceland's Thrihnukar volcano. This work-in-progress prototype addresses the lack of content and tools for immersive virtual reality (iVR) in geoscientific education and research and how to make it easier to integrate iVR into classroom experiences. It makes use of environmentally sensed data such that interaction and linked content can be integrated into a single experience. We discuss our workflows as well as methods and authoring tools for iVR analysis and creation of virtual educational experiences. These methods and tools aim to enhance the utility of geospatial data from repositories such as OpenTopography.org through unlocking treasure-troves of geospatial data for VR applications. Their enhanced accessibility in education and research for the geosciences and beyond will benefit geoscientists and educators who cannot be expected to be VR and 3D application experts.},
keywords={computer aided instruction;geophysics computing;optical radar;STEM;virtual reality;volcanology;iVR;geosciences;K12 STEM education;informal learning environment;immersive technologies;interactive volcano experience;LiDAR;light detection and ranging;Iceland Thrihnukar volcano;immersive virtual reality;geoscientific education;classroom experiences;environmentally sensed data;virtual educational experiences;geospatial data;OpenTopography.org;Geology;Three-dimensional displays;Laser radar;Atmospheric measurements;Extraterrestrial measurements;Particle measurements;Immersive virtual reality;geoscience education;volcano;3D modeling},
doi={10.1109/KELVAR.2017.7961557},
ISSN={null},
month={March},}
@ARTICLE{7138633,
author={C. {Resch} and P. {Keitler} and G. {Klinker}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Sticky Projections-A Model-Based Approach to Interactive Shader Lamps Tracking},
year={2016},
volume={22},
number={3},
pages={1291-1301},
abstract={Shader lamps can augment physical objects with projected virtual replications using a camera-projector system, provided that the physical and virtual object are well registered to each other. Precise registration and tracking has been a cumbersome and intrusive process in the past. In this paper, we present a new method for tracking complex-shaped physical objects interactively. In contrast to previous approaches our system is mobile and makes solely use of the projection of the virtual replication to track the physical object and “stick” the projection to it. Our method consists of two stages, a fast pose initialization based on structured light patterns and a non-intrusive frame-by-frame tracking based on features detected in the projection. During the tracking phase, a radiometrically corrected virtual camera view based on the current pose prediction is rendered and compared to the captured image. Matched features are triangulated providing a sparse set of surface points that is robustly aligned to the virtual model. The alignment transformation serves as an input for the new pose prediction. Detailed experiments including the evaluation of the overlay accuracy show that our approach can accurately and robustly track complex objects at interactive rates.},
keywords={augmented reality;cameras;computer vision;feature extraction;image matching;lamps;object tracking;rendering (computer graphics);virtual replication;structured light patterns;feature detection;radiometrically corrected virtual camera;augmented reality;computer vision;feature matching;pose prediction;rendering;virtual camera view;nonintrusive frame-by-frame tracking;complex-shaped physical object tracking;camera-projector system;interactive shader lamp tracking;model-based approach;sticky projection;Cameras;Three-dimensional displays;Tracking;Iterative closest point algorithm;Feature extraction;Radiometry;Calibration;Computer Vision;Augmented Reality.;Computer vision;augmented reality},
doi={10.1109/TVCG.2015.2450934},
ISSN={2160-9306},
month={March},}
@INPROCEEDINGS{7892384,
author={J. W. {Woodworth} and S. {Ekong} and C. W. {Borst}},
booktitle={2017 IEEE Virtual Reality (VR)},
title={Virtual field trips with networked depth-camera-based teacher, heterogeneous displays, and example energy center application},
year={2017},
volume={},
number={},
pages={471-472},
abstract={This demo presents an approach to networked educational virtual reality for virtual field trips and guided exploration. It shows an asymmetric collaborative interface in which a remote teacher stands in front of a large display and depth camera (Kinect) while students are immersed with HMDs. The teacher's front-facing mesh is streamed into the environment to assist students and deliver instruction. Our project uses commodity virtual reality hardware and high-performance networks to allow students who are unable to visit a real facility with an alternative that provides similar educational benefits. Virtual facilities can further be augmented with educational content through interactables or small games. We discuss motivation, features, interface challenges, and ongoing testing.},
keywords={augmented reality;cameras;computer aided instruction;distance learning;graphical user interfaces;groupware;helmet mounted displays;image capture;power engineering computing;virtual field trips;networked depth-camera-based teacher;heterogeneous displays;energy center application;networked educational virtual reality;guided exploration;asymmetric collaborative interface;remote teacher;HMD;commodity virtual reality hardware;high-performance networks;virtual facilities;educational content;augmented realities;Mirrors;Visualization;Virtual reality;Three-dimensional displays;Resists;Testing;Monitoring;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
doi={10.1109/VR.2017.7892384},
ISSN={2375-5334},
month={March},}
@ARTICLE{7950883,
author={E. {Patti} and A. {Mollame} and D. {Erba} and D. {Dalmasso} and A. {Osello} and E. {Macii} and A. {Acquaviva}},
journal={IT Professional},
title={Combining Building Information Modelling and Ambient Data in Interactive Virtual and Augmented Reality Environments},
year={2017},
volume={},
number={},
pages={1-1},
abstract={Smart Building is recent interdisciplinary research field that aims to improve the monitoring, management and maintenance of buildings. In this scenario, we present an innovative solution for combining BIM (Building Information Modelling) data with ambient information collected by heterogeneous devices deployed in the building. In order to collect environmental information, we exploit in this work a distributed software architecture. It enables the interoperability between heterogeneous data-sources, either physical devices like sensor nodes or third party software like Archibus, where building information resides. On top of this infrastructure, we developed an Android-based application that presents environmental building information integrated with BIM data in an Augmented and Virtual Reality environment. The proposed solution provides users awareness about building conditions and energy consumptions.},
keywords={Buildings;Interoperability;Middleware;Computer architecture;Object recognition;Architecture;Smart Building;Internet-of-Things;Virtual Reality;N. Learning Technologies;N.1 Learning environments;N.1.g Virtual and augmented reality;Augmented Reality;C Computer Systems Organization;C.2 Communication/Networking and Information Technology;C.2.4 Distributed Systems;BIM;Building Management;J Computer Applications;J.8 Internet Applications;J.8.l Middleware/business logic},
doi={10.1109/MITP.2017.265104553},
ISSN={1941-045X},
month={},}
@ARTICLE{8726075,
author={Q. {Tong} and X. {Li} and K. {Lin} and C. {Li} and W. {Si} and Z. {Yuan}},
journal={IEEE Network},
title={Cascade-LSTM-Based Visual-Inertial Navigation for Magnetic Levitation Haptic Interaction},
year={2019},
volume={33},
number={3},
pages={74-80},
abstract={Haptic feedback is crucial to immersive experience in virtual and augmented reality applications. The existing promising maglev haptic devices have advantages of no mechanical friction and low inertia. However, their performance is limited by the navigation approach, which mainly results from the challenge that it is difficult to obtain high precision, high frequency, and good stability with lightweight design at the same time. In this study, we reformulate visual-inertial navigation as a regression problem, and adopt deep learning to perform fusion navigation for maglev haptic interaction. A cascade-LSTM-based q-increment learning method is first proposed to progressively learn the increments of target variables. Two cascade LSTM networks are then constructed to estimate the increments of position and orientation, which are pipelined to accomplish visual-inertial fusion navigation. Additionally, we set up a maglev haptic platform as the system testbed. Experimental results show that our cascade-LSTMbased visual-inertial fusion navigation approach can reach 200 Hz while maintaining high-precision navigation (the mean absolute error of the position and orientation is less than 1 mm and 0.02°, respectively) for a maglev haptic interactive deformation application.},
keywords={augmented reality;haptic interfaces;inertial navigation;learning (artificial intelligence);magnetic levitation;recurrent neural nets;deep learning;maglev haptic interaction;cascade-LSTM-based q-increment learning method;cascade LSTM networks;maglev haptic platform;high-precision navigation;maglev haptic interactive deformation application;magnetic levitation haptic interaction;haptic feedback;immersive experience;virtual reality applications;augmented reality applications;mechanical friction;maglev haptic devices;cascade-LSTM-based visual-inertial fusion navigation approach;Navigation;Haptic interfaces;Visualization;Magnetic levitation;Magnetic resonance imaging;Learning systems;Probes},
doi={10.1109/MNET.2019.1800371},
ISSN={1558-156X},
month={May},}
@INPROCEEDINGS{8377777,
author={S. {Li} and Y. {Dou} and J. {Xu} and Q. {Wang} and X. {Niu}},
booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},
title={mmCNN: A Novel Method for Large Convolutional Neural Network on Memory-Limited Devices},
year={2018},
volume={01},
number={},
pages={881-886},
abstract={Deep learning recently has been widely used in many interactive application fields including but not limited to object recognition, speech recognition, natural language processing and so on. At the same time more and more attractive interactive applications (face recognition and augmented reality) are available on wearable and mobile devices. However, traditional deep learning methods such as CNN cost a lot of memory resources. This challenge makes it difficult to apply the powerful deep learning method on mobile memory limited platforms. In this paper we present a novel memory management strategy called mmCNN to solve this problem. This method helps us deploy a trained large size CNN on an any memory size platform including GPU, FPGA and memory-limited mobile devices. In our experiments, we run a feed-forward CNN process in an extremely small memory size (as low as 5MB) on a GPU platform. The result shows that our method saves more than 98% memory compared to a traditional CNN algorithm and further saves more than 90% compared to the sate-of-the-art related work "vDNN". Our work improve the computing scalability of interaction applications and break the memory bottleneck of using deep learning method on a memory-limited devices.},
keywords={augmented reality;convolution;face recognition;feedforward neural nets;field programmable gate arrays;graphics processing units;learning (artificial intelligence);mobile computing;natural language processing;object recognition;speech recognition;storage management;memory management strategy;memory size platform;feed-forward CNN process;GPU platform;memory bottleneck;memory-limited devices;mmCNN;convolutional neural network;interactive application fields;speech recognition;natural language processing;augmented reality;wearable devices;mobile devices;memory resources;mobile memory;deep learning methods;object recognition;face recognition;FPGA;vDNN;Memory management;Graphics processing units;Machine learning;Mobile handsets;Micromechanical devices;Runtime;Prefetching;cnn;gpu;memory management strategy;memory capacity limited},
doi={10.1109/COMPSAC.2018.00152},
ISSN={0730-3157},
month={July},}
@ARTICLE{8903228,
author={X. {Chen} and L. {Xu} and H. {Wei} and Z. {Shang} and T. {Zhang} and L. {Zhang}},
journal={IEEE Access},
title={Emotion Interaction Recognition Based on Deep Adversarial Network in Interactive Design for Intelligent Robot},
year={2019},
volume={7},
number={},
pages={166860-166868},
abstract={Augmented Reality devices (AR), virtual reality devices (VR), are changing our lives and it is critical to provide intelligent interaction and improve the user's intelligent interactive experience. When artificial intelligence is introduced into Intelligent interaction emotion classification or semantic segmentation and other tasks, it requires professional knowledge to manually label images sample. To address the problem of scarcity of labeled data in emotion classification, an improved classification method based on semi-supervised generative adversarial networks (GAN) is proposed in this paper. Firstly, the output layer of the traditional unsupervised GAN is replaced with Softmax layer to obtain the semi-supervised GAN. Secondly, additional labels are defined for generated samples to guiding the training process. Finally, we employ a semi-supervised training strategy to optimize the parameters of GAN and use the trained network to process videos. Experiments on existing public datasets show that our method has a certain improvement in compared with the classic methods based on deep learning, and has a higher recognition efficiency, which is more suitable for dimension emotion recognition of large-scale data.},
keywords={augmented reality;emotion recognition;image classification;intelligent robots;neural nets;robot vision;supervised learning;virtual reality;images sample;semantic segmentation;Intelligent interaction emotion classification;artificial intelligence;virtual reality devices;intelligent robot;interactive design;deep adversarial network;emotion interaction recognition;dimension emotion recognition;semisupervised training strategy;semisupervised GAN;Softmax layer;unsupervised GAN;semisupervised generative adversarial networks;Generative adversarial networks;Feature extraction;Emotion recognition;Training;Generators;Speech recognition;Intelligent robots;Emotional interaction;adversarial network;deep learning;softmax layer;artificial intelligence;interaction robot;semantic feature},
doi={10.1109/ACCESS.2019.2953882},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7939285,
author={M. A. {Zielke} and D. {Zakhidov} and G. {Hardee} and L. {Evans} and S. {Lenox} and N. {Orr} and D. {Fino} and G. {Mathialagan}},
booktitle={2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH)},
title={Developing Virtual Patients with VR/AR for a natural user interface in medical teaching},
year={2017},
volume={},
number={},
pages={1-8},
abstract={Professionalism and communication skills are important aspects of medical training, and virtual patient applications can offer cost effective, easily accessible platforms for communication practice which complement flexible, student-driven medical school curriculum design. Further, numerous virtual and augmented reality platforms have been introduced recently. This paper explores potential advantages and disadvantages Virtual and Augmented Reality (VR/AR) technologies offer to the development of a virtual patient application specifically for communication practice - the Emotive Virtual Patients - with a natural user interface. VR/AR technologies may offer highly interactive, immersive virtual patient experiences that tie to our research goals, improve presence and create a more fertile environment to practice empathy, however they may also present platform-specific challenges. A potential virtual patient design framework is discussed, and the unique benefits and limitations of VR/AR devices are analyzed. We put our research in the context of other virtual patient research, and hypothesize what benefits in terms of presence and natural user interfaces VR/AR may provide.},
keywords={augmented reality;biomedical education;medical computing;user interfaces;virtual patient development;natural user interface;medical teaching;empathetic communication;virtual patient applications;student-driven medical school curriculum design;virtual reality;augmented reality;VR-AR technologies;emotive virtual patients;VR-AR devices;virtual patient research;Education;Irrigation;virtual patients;virtual reality;augmented reality;natural interface;natural language processing;HoloLens},
doi={10.1109/SeGAH.2017.7939285},
ISSN={null},
month={April},}
@INPROCEEDINGS{7300743,
author={É. S. {Silva} and M. A. F. {Rodrigues}},
booktitle={2015 XVII Symposium on Virtual and Augmented Reality},
title={Gesture Interaction and Evaluation Using the Leap Motion for Medical Visualization},
year={2015},
volume={},
number={},
pages={160-169},
abstract={In this paper, we present and evaluate an interactive gesture controlled application using the Leap Motion for medical visualization, focusing on user satisfaction as an important component in the composition of the application success factors. Usability testings were conducted to verify important application requirements, among which, the asepsis in the working environment, accuracy of the interaction gestures, interaction time, level of interactivity, naturalness, effectiveness, ease of use and of learning, visual quality of the interface, utility, satisfaction and the non-occurrence of fatigue (physical and mental). The results show the effectiveness of the application in the recognition process of the modeled gestures and a very high level of overall satisfaction of the participants, indicating its strong potential as a touchless support tool in medical tasks guided by radiological images conducted in operating rooms.},
keywords={data visualisation;gesture recognition;medical computing;program testing;radiology;gesture interaction;leap motion;medical visualization;interactive gesture controlled application;user satisfaction;usability testings;application success factors;application requirernents;visual quality;touchless support tool;medical tasks;radiological irnages;operating rooms;Visualization;Three-dimensional displays;Solid modeling;Computational modeling;Biomedical imaging;Support vector machines;Usability;Gesture Interaction;Leap Motion;Usability Testing;Medical Visualization},
doi={10.1109/SVR.2015.31},
ISSN={null},
month={May},}
@ARTICLE{7468481,
author={A. {Stefanoiu} and A. {Weinmann} and M. {Storath} and N. {Navab} and M. {Baust}},
journal={IEEE Transactions on Image Processing},
title={Joint Segmentation and Shape Regularization With a Generalized Forward–Backward Algorithm},
year={2016},
volume={25},
number={7},
pages={3384-3394},
abstract={This paper presents a method for the simultaneous segmentation and regularization of a series of shapes from a corresponding sequence of images. Such series arise as time series of 2D images when considering video data, or as stacks of 2D images obtained by slicewise tomographic reconstruction. We first derive a model where the regularization of the shape signal is achieved by a total variation prior on the shape manifold. The method employs a modified Kendall shape space to facilitate explicit computations together with the concept of Sobolev gradients. For the proposed model, we derive an efficient and computationally accessible splitting scheme. Using a generalized forward-backward approach, our algorithm treats the total variation atoms of the splitting via proximal mappings, whereas the data terms are dealt with by gradient descent. The potential of the proposed method is demonstrated on various application examples dealing with 3D data. We explain how to extend the proposed combined approach to shape fields which, for instance, arise in the context of 3D+t imaging modalities, and show an application in this setup as well.},
keywords={computerised tomography;gradient methods;image reconstruction;image segmentation;image sequences;medical image processing;joint segmentation-shape regularization;generalized forward-backward algorithm;simultaneous segmentation;image sequence;2D images;slicewise tomographic reconstruction;shape signal regularization;modified Kendall shape space;Sobolev gradient concept;splitting scheme;proximal mappings;gradient descent;3D+t imaging modalities;Shape;Manifolds;Active contours;Image segmentation;Computational modeling;Three-dimensional displays;TV;Object segmentation;image sequence analysis},
doi={10.1109/TIP.2016.2567068},
ISSN={1941-0042},
month={July},}
@INPROCEEDINGS{8943688,
author={J. {Collins} and H. {Regenbrecht} and T. {Langlotz} and Y. {Said Can} and C. {Ersoy} and R. {Butson}},
booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
title={Measuring Cognitive Load and Insight: A Methodology Exemplified in a Virtual Reality Learning Context},
year={2019},
volume={},
number={},
pages={351-362},
abstract={Recent improvements of Virtual Reality (VR) technology have enabled researchers to investigate the benefits VR may provide for various domains such as health, entertainment, training, and education. A significant proportion of VR system evaluations rely on perception-based measures such as user pre-and post-questionnaires and interviews. While these self-reports provide valuable insights into users' perceptions of VR environments, recent developments in digital sensors and data collection techniques afford researchers access to measures of physiological response. This work explores the merits of physiological measures in the evaluation of emotional responses in virtual environments (ERVE). We include and place at the center of our ERVE methodology emotional response data by way of electrodermal activity and heart-rate detection which are analyzed in conjunction with event-driven data to derive further measures. In this paper, we present our ERVE methodology together with a case study within the context of VR-based learning in which we derive measures of cognitive load and moments of insight. We discuss our methodology, and its potential for use in many other application and research domains to provide more in-depth and objective analyses of experiences within VR.},
keywords={Virtual Reality;Interactive Learning Environments;Methodology},
doi={10.1109/ISMAR.2019.00033},
ISSN={1554-7868},
month={Oct},}
@INPROCEEDINGS{7986319,
author={L. {Abdi} and W. {Takrouni} and A. {Meddeb}},
booktitle={2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC)},
title={In-vehicle cooperative driver information systems},
year={2017},
volume={},
number={},
pages={396-401},
abstract={Critical traffic problems such as accidents and traffic congestion require the development of new transportation systems. Research in perceptual and human factors assessment is needed for relevant and correct display of this information for maximal road traffic safety as well as optimal driver comfort. One of the solutions to prevent accidents is to provide information on the surrounding environment of the driver. The development and deployment of cooperative vehicular safety systems undeniably require a combination of dedicated wireless communications, computer vision, and AR technologies as the building blocks of cooperative safety systems. Augmented Reality Head-Up Display (AR-HUD) can facilitate a new form of dialogue between the vehicle and the driver; and enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads. In this paper, we propose a fast deep-learning-based object detection approaches for identifying and recognizing road obstacles types, as well as interpreting and predicting complex traffic situations. A single Convolutional Neural Network (CNN) predicts region of interest and class probabilities directly from full images in one evaluation. We also investigated potential costs and benefits of using dynamic conformal AR cues in improving driving safety. A new AR-HUD approach to create real-time interactive traffic animations was introduced in terms of types of obstacle, rules for placement and visibility, and projection of these on an in-vehicle display.},
keywords={augmented reality;cooperative communication;driver information systems;head-up displays;neural nets;road safety;road traffic;in-vehicle cooperative driver information systems;road traffic safety;cooperative vehicular safety systems;augmented reality head-up display;fast deep-learning-based object detection;convolutional neural network;Vehicles;Roads;Safety;Object detection;Streaming media;Computer vision;Automotive components;Transportation Systems;Cooperative Vehicular Safety systems;Augmented Reality Head-Up Display;Deep Learning;Convolutional Neural Network},
doi={10.1109/IWCMC.2017.7986319},
ISSN={2376-6506},
month={June},}
@ARTICLE{8750805,
author={F. {Strada} and A. {Bottino} and F. {Lamberti} and G. {Mormando} and P. L. {Ingrassia}},
journal={IEEE Transactions on Emerging Topics in Computing},
title={Holo-BLSD - A Holographic Tool for Self-training and Self-evaluation of Emergency Response Skills},
year={2019},
volume={},
number={},
pages={1-1},
abstract={In case of cardiac arrest, prompt intervention of bystanders can be vital in saving lives. Basic Life Support and Defibrillation (BLSD) is a procedure designed to deliver a proficient emergency first response. Developing skills in BLSD in a large part of the population is a primary educational goal of resuscitation medicine. In this context, novel computer science technologies like Augmented Reality (AR) and Virtual Reality (VR) can alleviate some of the drawbacks of traditional instructor-led courses, especially concerning time and cost constraints. This paper presents Holo-BLSD, an AR system that allows users to learn and train the different operations involved in BLSD and receive an automatic assessment. The system uses a standard manikin which is \quotes{augmented} by an interactive virtual environment that reproduces realistic emergency scenarios. The proposed approach has been validated through a user study. Subjective results confirmed the usability of the devised tool and its capability to stimulate learners' attention. Objective results indicated no statistical significance in the differences between the examiners' evaluation of users who underwent traditional and AR training; they also showed a close agreement between expert and automatic assessments, suggesting that Holo-BLSD can be regarded as an effective self-learning method and a reliable self-evaluation tool.},
keywords={Basic Life Support and Defibrillation (BLSD);self-learning;self-evaluation;Augmented Reality;user study},
doi={10.1109/TETC.2019.2925777},
ISSN={2376-4562},
month={},}
@INPROCEEDINGS{7413895,
author={A. {Pozzebon} and S. {Calamai}},
booktitle={2015 Digital Heritage},
title={Smart devices for Intangible Cultural Heritage fruition},
year={2015},
volume={1},
number={},
pages={333-336},
abstract={This paper proposes a novel approach to the fruition of Intangible Cultural Heritage exploiting the technical features of smart devices. In particular, it presents a framework for a “sound tourism”, in which the perception of sites is directly transmitted by the voice of the local communities through the creation of an app model for the fruition of landscape, places, and locations by means of oral archives. That is, the app model aims at boosting the added value of Intangible Cultural Heritage (e.g. popular music, oral history, languages and accents, local tradition, and folklore) by re-using it in real application environments (e.g. for tourist purposes). The proposed solution rejects the use of a smartphone as a mere displayer of information embracing an innovative philosophy that considers the smart device as a tool for the playback of audio material stored in sound archives, leaving the user free to enjoy the site he/she is visiting without the need to interact with a screen.},
keywords={audio recording;augmented reality;Bluetooth;Global Positioning System;history;information retrieval systems;interactive systems;mobile computing;near-field communication;travel industry;intangible cultural heritage fruition;smart devices;sound tourism;technical features;local communities;oral archives;innovative philosophy;audio material playback;sound archives;GPS;near field communication;bluetooth beacons;Cultural differences;Global communication;Mobile handsets;Augmented reality;History;Europe;Intangible Cultural Heritage;sound archives;smart devices;storytelling;app},
doi={10.1109/DigitalHeritage.2015.7413895},
ISSN={null},
month={Sep.},}
@INPROCEEDINGS{8975993,
author={X. {Huang} and J. {Twycross} and F. {Wild}},
booktitle={2019 International Conference on 3D Immersion (IC3D)},
title={A Process for the Semi-Automated Generation of Life-Sized, Interactive 3D Character Models for Holographic Projection},
year={2019},
volume={},
number={},
pages={1-8},
abstract={By mixing digital data into the real world, Augmented Reality (AR) can deliver potent immersive and interactive experience to its users. In many application contexts, this requires the capability to deploy animated, high fidelity 3D character models. In this paper, we propose a novel approach to efficiently transform – using 3D scanning – an actor to a photorealistic, animated character. This generated 3D assistant must be able to move to perform recorded motion capture data, and it must be able to generate dialogue with lip sync to naturally interact with the users. The approach we propose for creating these virtual AR assistants utilizes photogrammetric scanning, motion capture, and free viewpoint video for their integration in Unity. We deploy the Occipital Structure sensor to acquire static high-resolution textured surfaces, and a Vicon motion capture system to track series of movements. The proposed capturing process consists of the steps scanning, reconstruction with Wrap 3 and Maya, editing texture maps to reduce artefacts with Photoshop, and rigging with Maya and Motion Builder to render the models fit for animation and lip-sync using LipSyncPro. We test the approach in Unity by scanning two human models with 23 captured animations each. Our findings indicate that the major factors affecting the result quality are environment setup, lighting, and processing constraints.},
keywords={3D scanning;reconstruction;motion capture;Augmented Reality},
doi={10.1109/IC3D48390.2019.8975993},
ISSN={2379-1772},
month={Dec},}
@INPROCEEDINGS{8802389,
author={R. S. {Cavalcante} and E. A. {Lamounier Júnior} and A. {Cardoso} and A. {Soares} and G. M. d. {Lima}},
booktitle={2018 20th Symposium on Virtual and Augmented Reality (SVR)},
title={Development of a Serious Game for Rehabilitation of Upper Limb Amputees},
year={2018},
volume={},
number={},
pages={99-105},
abstract={The development of virtual environments as support tools in rehabilitation processes has proved to be valid and important for the users, providing a special and fun way to execute the procedures. The more the user feels immersed and motivated in the virtual environment, the less he or she is likely to give up the rehabilitation process. There is also a better chance of getting good levels of use. The application of serious gaming features in virtual environment makes it favorable to create a system that allows the user to evolve in their rehabilitation process, as they enjoy the tasks and challenges proposed. However, the development of such a system requires good decisions concerning the interaction devices, the 3D modeling of the environment and the definition of the tasks and the challenges of the game. In this context, this work proposes the development of a virtual training environment for amputees to minimize their time to adapt to a real prosthesis, using a tether with different sensor to interact with the virtual environment. The training protocols were provided by health-care professionals and the interaction technology was developed under their supervision, to ensure high levels of mobility and comfort for the user.},
keywords={biomedical education;computer based training;health care;interactive devices;medical computing;patient rehabilitation;prosthetics;serious games (computing);solid modelling;virtual reality;virtual environment;rehabilitation process;virtual training environment;serious game;upper limb amputees rehabilitation;3D modeling;prosthesis;healthcare professionals;Virtual Environment;Amputees;Upper Limbs;Training},
doi={10.1109/SVR.2018.00025},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{8526423,
author={C. {Sun} and P. {Chiang}},
booktitle={2018 IEEE XXV International Conference on Electronics, Electrical Engineering and Computing (INTERCON)},
title={Mr. Piano: A Portable Piano Tutoring System},
year={2018},
volume={},
number={},
pages={1-4},
abstract={We present a portable piano-tutoring system that facilitates piano learning without the spatial limitations and unaffordable cost. Our proposed system is able to simultaneously project an interactive piano on the table while displaying a music sheet with animated instruction on the smartphone. By detecting the user's hand movements with our efficient approach, visual and sound feedbacks can be given in real time. Mr. Piano gives instructions and feedbacks that helps a beginner to learn fundamental piano skills. The user can play with demonstration, instruction, or practice mode with different level of instructions. Unlike other mobile piano-tutoring system, the key of our projected piano is equivalent in size to a standard one that makes users adjust themselves to play a actual piano better. The user evaluation shows our proposed system helps users to learn piano effectively.},
keywords={computer animation;feedback;intelligent tutoring systems;music;musical instruments;smart phones;fundamental piano skills;mobile piano-tutoring system;actual piano;portable piano tutoring system;piano learning;interactive piano;music sheet;instructions animation;smartphone;users hand movements;sound feedbacks;visual feedbacks;piano projection;Keyboards;Real-time systems;Image color analysis;Visualization;Webcams;Skin;Instruments;augmented reality;human computer interaction;piano learning interface},
doi={10.1109/INTERCON.2018.8526423},
ISSN={null},
month={Aug},}
@INPROCEEDINGS{7740300,
author={},
booktitle={2016 6th International Conference on IT Convergence and Security (ICITCS)},
title={Table of contents},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The following topics are dealt with: power-aware data structure; memory allocation techniques; EPS-based motion recognition systems; secure distance bounding protocol; TH-UWB; cooperative spectrum sensing scheme; femtocell network; video quality; unequal loss protection; Wi-Fi based broadcasting system; example-based retrieval system; human motion data; astronaut virtual training system; layout familiarization training; stereo image correction; 3D optical microscope; UHF RFID; BLE; stereo-based tag association; medical image segmentation; sensitive adaptive thresholding; interactive event recognition; semantic video understanding; Bgslibrary algorithms; traffic surveillance video; CUDA-based acceleration techniques; image filtering; image-based ship detection; AR navigation; augmented reality; vehicle information; head-up display: LiDAR data; classifier performance; people detection; wavelet transform; max-min energy-efficiency optimization; wireless powered communication network; harvest-then-transmit protocol; ARM64bit Server; WeChat text messages service flow traffic classification; machine learning technique; load balancing; WSN; novel Markov decision process based routing algorithm; repulsion-propulsion firefly algorithm; sentence based mathematical problem solving approach; ontology modeling; sentiment analysis; HARN algorithm; real-time road surface condition determination algorithm; automatic weather system; kinematic constraint method; human gesture recognition; weighted dynamic time warping; IT demand governance; business goal structuring notation; software requirement specification; AOP-based approach;decision support system; proactive flood control; context-aware user interface field classification; common vocabulary set; maritime equipment; e-navigation services; genetic algorithm; strategic information systems planning; speech enhancement; ES information; phase-error based filters; holistic service orchestration; distributed micro data center; hierarchical cluster network; wellness sports industry; secure agent based architecture; resource allocation; cloud computing; distributed multi-platform context-aware user interface; parallel prime number labeling; XML data; MapReduce; metadata extension; data presentations; aspect-oriented user interfaces design integration; Angular 2 framework; energy impact; Web user interface technology; mobile devices; JIT compilation-based unified SQL query optimization system; partial materialization; data integration; SQL-on-Hadoop engines; z-transform based encryption algorithm; FARIS; fast and memory-efficient URL filter; domain specific machine; synchronized blind audio watermarking; multilevel DWT; windowed vector modulation; packet length covert channel capacity estimation; flexible authentication protocol; WBAN; SMS-based mobile botnet detection module; full-duplex jamming attack; active eavesdropping; Big Data security analysis; complex security requirements patterns; SSH attacks; SSL/TLS nonintrusive proxy; JSON data; neural stegoclassifier; polymorphic malware detection; linguistic based steganography; lexical substitution; syntactical transformation; holistic-based feature extraction; error correcting code biometric template protection technique; network based IMSI catcher detection; Internet of Things environment; light-weight API-call safety checking; automotive control software; SmartDriver; project management software; model-based testing; exploratory testing; automated ECG beat classification system and convolutional neural networks.},
keywords={application program interfaces;aspect-oriented programming;astronomy computing;audio watermarking;augmented reality;Big Data;biometrics (access control);body area networks;channel capacity;cloud computing;computer based training;cooperative communication;cryptographic protocols;data integration;data structures;decision support systems;discrete wavelet transforms;driver information systems;electrocardiography;electronic messaging;energy conservation;error correction codes;feature extraction;femtocellular radio;formal specification;genetic algorithms;gesture recognition;image filtering;image segmentation;information systems;Internet of Things;invasive software;learning (artificial intelligence);Markov processes;medical image processing;meta data;neural nets;object detection;ontologies (artificial intelligence);optical radar;parallel programming;power aware computing;problem solving;program testing;query processing;radiofrequency identification;resource allocation;sentiment analysis;signal classification;signal detection;speech enhancement;sport;SQL;stereo image processing;storage management;strategic planning;ubiquitous computing;ultra wideband communication;user interfaces;video surveillance;vocabulary;wireless LAN;wireless sensor networks;XML;power-aware data structure;memory allocation techniques;EPS-based motion recognition systems;secure distance bounding protocol;TH-UWB;cooperative spectrum sensing scheme;femtocell network;video quality;unequal loss protection;Wi-Fi based broadcasting system;example-based retrieval system;human motion data;astronaut virtual training system;layout familiarization training;stereo image correction;3D optical microscope;UHF RFID;BLE;stereo-based tag association;medical image segmentation;sensitive adaptive thresholding;interactive event recognition;semantic video understanding;Bgslibrary algorithms;traffic surveillance video;CUDA-based acceleration techniques;image filtering;image-based ship detection;AR navigation;augmented reality;vehicle information;head-up display;LiDAR data;classifier performance;people detection;wavelet transform;max-min energy-efficiency optimization;wireless powered communication network;harvest-then-transmit protocol;ARM64bit Server;WeChat text messages service flow traffic classification;machine learning technique;load balancing;WSN;Markov decision process based routing algorithm;repulsion-propulsion firefly algorithm;holistic service orchestration;distributed micro data center;hierarchical cluster network;wellness sports industry;secure agent based architecture;resource allocation;cloud computing;distributed multi-platform context-aware user interface;parallel prime number labeling;XML data;MapReduce;metadata extension;data presentations;aspect-oriented user interfaces design integration;Angular 2 framework;energy impact;Web user interface technology;mobile devices;JIT compilation-based unified SQL query optimization system;partial materialization;data integration;SQL-on-Hadoop engines;z-transform based encryption algorithm;FARIS;fast and memory-efficient URL filter;domain specific machine;synchronized blind audio watermarking;multilevel DWT;windowed vector modulation;packet length covert channel capacity estimation;flexible authentication protocol;WBAN;SMS-based mobile botnet detection module;phase-error based filters;ES information;speech enhancement;strategic information systems planning;genetic algorithm;e-navigation services;maritime equipment;common vocabulary set;context-aware user interface field classification;proactive flood control;decision support system;AOP-based approach;software requirement specification;business goal structuring notation;IT demand governance;weighted dynamic time warping;human gesture recognition;kinematic constraint method;automatic weather system;real-time road surface condition determination algorithm;HARN algorithm;sentiment analysis;ontology modeling;sentence based mathematical problem solving approach;full-duplex jamming attack;active eavesdropping;Big Data security analysis;complex security requirements patterns;SSH attacks;SSL/TLS nonintrusive proxy;JSON data;neural stegoclassifier;polymorphic malware detection;linguistic based steganography;lexical substitution;syntactical transformation;holistic-based feature extraction;error correcting code biometric template protection;network based IMSI catcher detection;convolutional neural networks;automated ECG beat classification system;exploratory testing;model-based testing;project management software;SmartDriver;automotive control Software;light-weight API-call safety checking;Internet of Things environment},
doi={10.1109/ICITCS.2016.7740300},
ISSN={null},
month={Sep.},}
@INPROCEEDINGS{7863145,
author={},
booktitle={2016 22nd International Conference on Virtual System Multimedia (VSMM)},
title={[Front matter]},
year={2016},
volume={},
number={},
pages={I-X},
abstract={The following topics are dealt with: virtual system; multimedia; architectural history; privacy preservation; medical image; virtual reality; interactive application; patient treatment; Google Glass; AMbiART; cultural heritage; MMO players; independent learning environment; knowledge transfer; drones; serious games; museum; emergency management; elderly; 3D model; digital archive; augmented reality system; location-aware media; image compression; and gamification.},
keywords={computer aided instruction;computer games;computer graphics;data privacy;geriatrics;humanities;information retrieval systems;knowledge management;medical image processing;patient treatment;architectural history;privacy preservation;medical image;virtual reality;interactive application;patient treatment;Google Glass;AMbiART;cultural heritage;MMO players;independent learning environment;knowledge transfer;drones;serious games;emergency management;elderly;3D model;digital archive;augmented reality system;location-aware media;image compression;gamification;museum},
doi={10.1109/VSMM.2016.7863145},
ISSN={2474-1485},
month={Oct},}
